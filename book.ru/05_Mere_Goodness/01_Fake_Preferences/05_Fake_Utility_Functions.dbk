<section xml:id="item_NnohDYHNnKDtbiMyp"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>Fake Utility Functions</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/NnohDYHNnKDtbiMyp/fake-utility-functions"></link>
    </bibliosource>
    <pubdate doc_status="draft">2007-12-06</pubdate>
  </info>
  <indexterm><primary>Utility Functions</primary></indexterm>
<indexterm><primary>Complexity of Value</primary></indexterm>
  <para>Every now and then, you run across someone who has discovered the One Great Moral Principle, of which all other values are a mere derivative consequence.</para>
<para>I run across more of these people than you do.  Only in my case, it&apos;s people who know <emphasis>the amazingly simple utility function that is all you need to program into an artificial superintelligence</emphasis> and then everything will turn out fine.</para>
<para>(This post <emphasis>should</emphasis> come as an anticlimax, since you already know virtually all the concepts involved, <emphasis>I bloody well hope.</emphasis>  See <link xl:href="https://www.lesswrong.com/posts/D6rsNhHM4pBCpDzSb/fake-fake-utility-functions">yesterday&apos;s post</link>, and all my posts since <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_bfbiyTogEKWEGP96S">October 31st</olink>, actually...)</para>
<para>Some people, when they encounter the how-to-program-a-superintelligence problem, try to solve the problem immediately.  Norman R. F. Maier:  &quot;<olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_uHYYA32CKgKT3FagE">Do not propose solutions</olink> until the problem has been discussed as thoroughly as possible without suggesting any.&quot;  Robyn Dawes:  &quot;I have often used this edict with groups I have led - particularly when they face a very tough problem, which is when group members are most apt to propose solutions immediately.&quot;  Friendly AI is an <emphasis>extremely</emphasis> tough problem so people solve it <emphasis>extremely</emphasis> fast. </para>
<para>There&apos;s several major classes of fast wrong solutions I&apos;ve observed; and one of these is the Incredibly Simple Utility Function That Is All A Superintelligence Needs For Everything To Work Out Just Fine.</para>
<para>I may have contributed to this problem with a really poor choice of phrasing, years ago when I first started talking about &quot;Friendly AI&quot;.  I referred to the <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_n5ucT5ZbPdhfGNLtP">optimization criterion</olink> of an optimization process - the region into which an agent tries to steer the future - as the &quot;supergoal&quot;.  I&apos;d <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_sSqoEw9eRP2kPKLCz">meant</olink> &quot;super&quot; in the sense of &quot;parent&quot;, the source of a directed link in an acyclic graph.  But it seems the effect of my phrasing was to send some people into <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_XrzQW69HpidzvBxGr">happy death spirals</olink> as they tried to imagine the Superest Goal Ever, the Goal That Overrides All Over Goals, the Single Ultimate Rule From Which All Ethics Can Be Derived.</para>
<para>But a utility function doesn&apos;t have to be simple.  It can contain an arbitrary number of terms.  We have every reason to believe that insofar as humans can said to be have values, there are lots of them - high <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_f4txACqDWithRi7hs">Kolmogorov complexity</olink>.  A human brain implements a thousand shards of desire, though this fact may not be appreciated by one who has not studied <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_epZLSoNvjW53tqNj9">evolutionary psychology</olink>.  (Try to explain this without a <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_HLqWn5LASfhhArZ7w">full, long introduction</olink>, and the one hears &quot;humans are trying to maximize fitness&quot;, which is exactly the <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_XPErvb8m9FapXCjhA">opposite</olink> of what evolutionary psychology says.)</para>
<para>So far as <link xl:href="http://atheism.about.com/library/FAQs/phil/blfaq_phileth_cat.htm">descriptive</link> theories of morality are concerned, the complicatedness of human morality is a <emphasis>known fact.</emphasis>  It is a <emphasis>descriptive</emphasis> fact about human beings, that the love of a parent for a child, and the love of a child for a parent, and the love of a man for a woman, and the love of a woman for a man, have not been cognitively derived from each other or from any other value.  A mother doesn&apos;t have to do complicated moral philosophy to love her daughter, nor extrapolate the consequences to some other desideratum.  There are many such shards of desire, all <emphasis>different</emphasis> values.</para>
<para>Leave out just <emphasis>one</emphasis> of these values from a superintelligence, and even if you successfully include <emphasis>every other</emphasis> value, you could end up with a <link xl:href="http://www.nickbostrom.com/existential/risks.html">hyperexistential catastrophe</link>, a fate worse than death.  If there&apos;s a superintelligence that wants everything for us that we want for ourselves, <emphasis>except</emphasis> the human values relating to controlling your own life and achieving your own goals, that&apos;s one of the oldest dystopias in the <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_rHBdcHGLJ7KvLJQPk">book</olink>.  (Jack Williamson&apos;s &quot;With Folded Hands&quot;, in this case.)</para>
<para>So how does the one constructing the Amazingly Simple Utility Function deal with this objection?</para>
<para>Objection?  <emphasis>Objection?</emphasis>  Why would they be searching for possible <emphasis>objections</emphasis> to their lovely theory?  (Note that the process of searching for <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_dHQkDNMhj692ayx78">real, fatal objections</olink> isn&apos;t the same as performing a <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_3nZMgRTfFEfHp34Gb">dutiful search</olink> that amazingly hits on only questions to which they have a snappy answer.)  They don&apos;t know any of this stuff.  They aren&apos;t thinking about burdens of proof.  They don&apos;t know the problem is difficult.  They heard the word &quot;supergoal&quot; and went off in a <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_XrzQW69HpidzvBxGr">happy death spiral</olink> around &quot;<olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_kpRSCH7ALLcb6ucWM">complexity</olink>&quot; or whatever.</para>
<para>Press them on some particular point, like the love a mother has for her children, and they reply &quot;But if the superintelligence wants &apos;complexity&apos;, it will see how complicated the parent-child relationship is, and therefore encourage mothers to love their children.&quot;  Goodness, where do I start?</para>
<para>Begin with the <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_L32LHWzy9FzSDazEg">motivated stopping</olink>:  A superintelligence actually searching for ways to maximize complexity wouldn&apos;t conveniently stop if it noticed that a parent-child relation was complex.  It would ask if anything else was <emphasis>more</emphasis> complex.  This is a <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_bfbiyTogEKWEGP96S">fake justification</olink>; the one trying to argue the imaginary superintelligence into a policy selection, didn&apos;t really arrive at that policy proposal by carrying out a <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_i6fKszWY6gLZSX2Ey">pure search</olink> for ways to maximize complexity.</para>
<para>The whole argument is a <olink targetdoc="item_T3CjiQq6bBgFuzvp6" targetptr="item_fATPBv4pnHC33EmJ2">fake morality</olink>.  If what you <emphasis>really</emphasis> valued was complexity, then you would be justifying the parental-love drive by pointing to how it increases complexity.  If you justify a complexity drive by alleging that it increases parental love, it means that what you really value is the parental love.  It&apos;s like giving a prosocial argument in favor of selfishness.</para>
<para>But if you consider the affective death spiral, then it doesn&apos;t increase the perceived niceness of &quot;complexity&quot; to say &quot;A mother&apos;s relationship to her daughter is only important because it increases complexity; consider that if the relationship became simpler, we would not value it.&quot;  What does increase the perceived niceness of &quot;complexity&quot; is saying, &quot;If you set out to increase complexity, mothers will love their daughters - look at the positive consequence this has!&quot;</para>
<para>This point applies whenever you run across a moralist who tries to convince you that their One Great Idea is all that anyone needs for moral judgment, and proves this by saying, &quot;Look at all these positive consequences of this Great Thingy&quot;, rather than saying, &quot;Look at how all these things we think of as &apos;positive&apos; are only positive when their consequence is to increase the Great Thingy.&quot;  The latter being what you&apos;d actually need to carry such an argument.</para>
<para>But if you&apos;re trying to persuade others (or yourself) of your theory that the One Great Idea is &quot;bananas&quot;, you&apos;ll sell a lot more bananas by arguing how bananas lead to better sex, rather than claiming that you should only want sex when it leads to bananas.</para>
<para>Unless you&apos;re so far gone into the Happy Death Spiral that you really <emphasis>do</emphasis> start saying &quot;Sex is only good when it leads to bananas.&quot;  Then you&apos;re in trouble.  But at least you won&apos;t convince anyone else.</para>
<para>In the end, the only process that reliably <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_fg9fXrHpeaDD6pEPL">regenerates</olink> all the local decisions you would make given your morality, is your morality.  Anything else - any attempt to substitute instrumental means for terminal ends - ends up <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_sP2Hg6uPwpfp3jZJN">losing purpose</olink> and requiring <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_YhgjmCxcQXixStWMC">an infinite number of patches</olink> because the system doesn&apos;t <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_fg9fXrHpeaDD6pEPL">contain the source</olink> of the instructions you&apos;re giving it.  You shouldn&apos;t expect to be able to compress a human morality down to a simple utility function, any more than you should expect to compress a large computer file down to 10 bits.</para>
<para><emphasis role="bold">Addendum:</emphasis>  Please note that we&apos;re not yet ready to discuss Friendly AI, as such, on <emphasis>Overcoming Bias.</emphasis>  That will require <emphasis>a lot more</emphasis> prerequisite material.  This post is <emphasis>only </emphasis>about why simple utility functions fail to compress our values.</para>

  
</section>
