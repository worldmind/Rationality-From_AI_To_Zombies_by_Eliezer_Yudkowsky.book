<section xml:id="item_PtoQdG7E8MxYJrigu"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>No Universally Compelling Arguments</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/PtoQdG7E8MxYJrigu/no-universally-compelling-arguments"></link>
    </bibliosource>
    <pubdate doc_status="draft">2020-09-16</pubdate>
  </info>
  <indexterm><primary>Metaethics</primary></indexterm>
<indexterm><primary>AI</primary></indexterm>
  <para>What is so <emphasis>terrifying</emphasis> about the idea that not every possible mind might agree with us, even in principle?</para>
<para>For some folks, nothing—it doesn&apos;t bother them in the slightest. And for some of <emphasis>those</emphasis> folks, the <emphasis>reason</emphasis> it doesn&apos;t bother them is that they don&apos;t have strong intuitions about standards and truths that go beyond personal whims.  If they say the sky is blue, or that murder is wrong, that&apos;s just their personal opinion; and that someone else might have a different opinion doesn&apos;t surprise them.</para>
<para>For other folks, a disagreement that persists even <emphasis>in principle</emphasis> is something they can&apos;t accept.  And for some of <emphasis>those</emphasis> folks, the <emphasis>reason</emphasis> it bothers them, is that it seems to them that if you allow that some people cannot be persuaded <emphasis>even in principle</emphasis> that the sky is blue, then you&apos;re conceding that &quot;the sky is blue&quot; is merely an <emphasis>arbitrary</emphasis> personal opinion.</para>
<para><olink targetdoc="item_T3CjiQq6bBgFuzvp6" targetptr="item_tnWRXkcDi5Tw9rzXw">Yesterday</olink>, I proposed that you should resist the temptation to generalize over all of mind design space.  If we restrict ourselves to minds specifiable in a trillion bits or less, then each <emphasis>universal</emphasis> generalization &quot;All minds m: X(m)&quot; has two to the trillionth chances to be false, while each <emphasis>existential</emphasis> generalization &quot;Exists mind m: X(m)&quot; has two to the trillionth chances to be true.</para>
<para>This would seem to argue that for every argument A, howsoever convincing it may seem to us, there exists at least one possible mind that doesn&apos;t buy it.</para>
<para>And the surprise and/or horror of this prospect (for some) has a great deal to do, I think, with the intuition of the <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_cnYHFNBF3kZEyx24v">ghost-in-the-machine</olink>—a ghost with some irreducible core that any <emphasis>truly valid</emphasis> argument will convince.</para>
<para>I have <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_cnYHFNBF3kZEyx24v">previously spoken</olink> of the intuition whereby people <link xl:href="https://www.lesswrong.com/posts/6ByPxcGDhmx74gPSm/surface-analogies-and-deep-causes">map</link><emphasis>programming a computer</emphasis>, onto <emphasis>instructing a human servant</emphasis>, so that the computer might rebel against its code—or perhaps look over the code, decide it is not reasonable, and hand it back.</para>
<para>If there were a ghost in the machine and the ghost contained an irreducible core of reasonableness, above which any mere code was only a suggestion, then there might be universal arguments.  Even if the ghost was initially handed code-suggestions that contradicted the Universal Argument, then when we finally did expose the ghost to the Universal Argument—or the ghost could discover the Universal Argument on its own, that&apos;s also a popular concept—the ghost would just override its own, mistaken source code.</para>
<para>But as the student programmer once said, &quot;I get the feeling that the computer just skips over all the comments.&quot;  The code is not given to the AI; the code <emphasis>is</emphasis> the AI.</para>
<para>If you switch to the physical perspective, then the notion of a Universal Argument seems noticeably unphysical.  If there&apos;s a physical system that at time T, after being exposed to argument E, does X, then there ought to be another physical system that at time T, after being exposed to environment E, does Y.  Any thought has to be implemented <emphasis>somewhere</emphasis>, in a physical system; any belief, any conclusion, any decision, any motor output.  For every lawful causal system that zigs at a set of points, you should be able to specify another causal system that lawfully zags at the same points.</para>
<para>Let&apos;s say there&apos;s a mind with a transistor that outputs +3 volts at time T, indicating that it has just assented to some persuasive argument.  Then we can build a highly similar physical cognitive system with a tiny little trapdoor underneath the transistor containing a little grey man who climbs out at time T and sets that transistor&apos;s output to—3 volts, indicating non-assent.  Nothing acausal about that; the little grey man is there because we built him in.  The notion of an argument that convinces <emphasis>any</emphasis> mind seems to involve a little blue woman who was <emphasis>never</emphasis> built into the system, who climbs out of literally <emphasis>nowhere,</emphasis> and strangles the little grey man, because that transistor has just <emphasis>got</emphasis> to output +3 volts:  It&apos;s such a <emphasis>compelling argument,</emphasis> you see.</para>
<para>But compulsion is not a property of arguments, it is a <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_ZTRiSNmeGQK8AkdN2">property of minds</olink> that process arguments.</para>
<para>So the reason I&apos;m arguing against the ghost, isn&apos;t <emphasis>just</emphasis> to make the point that (1) Friendly AI has to be explicitly programmed and (2) the laws of physics do not forbid Friendly AI. (Though of course I take a certain interest in establishing this.)</para>
<para>I also wish to establish the notion of a mind as a <emphasis>causal, lawful, physical system</emphasis> in which there <emphasis>is no</emphasis> irreducible central ghost that looks over the neurons / code and decides whether they are good suggestions.</para>
<para>(There is a concept in Friendly AI of <emphasis>deliberately</emphasis> programming an FAI to review its own source code and possibly hand it back to the programmers.  But the mind that reviews is not irreducible, it is just the mind that you created.  The FAI is renormalizing itself <emphasis>however it was designed to do so;</emphasis> there is nothing acausal reaching in from outside.  A bootstrap, not a skyhook.)</para>
<para>All this echoes back to the <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_qmqLxvtsPzZ2s6mpY">discussion</olink>, a good deal earlier, of a Bayesian&apos;s &quot;arbitrary&quot;<link xl:href="https://www.lesswrong.com/posts/jzf4Rcienrm6btRyt/priors-as-mathematical-objects">priors</link>.  If you show me one Bayesian who draws 4 red balls and 1 white ball from a barrel, and who assigns probability 5/7 to obtaining a red ball on the next occasion (by Laplace&apos;s Rule of Succession), then I can show you <link xl:href="https://www.lesswrong.com/posts/jzf4Rcienrm6btRyt/priors-as-mathematical-objects">another mind</link> which obeys Bayes&apos;s Rule to conclude a 2/7 probability of obtaining red on the next occasion—corresponding to a different prior belief about the barrel, but, perhaps, a less &quot;reasonable&quot; one.</para>
<para>Many philosophers are convinced that because you can in-principle construct a prior that updates to any given conclusion on a stream of evidence, therefore, Bayesian reasoning must be &quot;arbitrary&quot;, and the whole schema of Bayesianism flawed, because it relies on &quot;unjustifiable&quot; assumptions, and indeed &quot;unscientific&quot;, because you cannot force any possible journal editor in mindspace to agree with you.</para>
<para>And this (I then replied) relies on the notion that by unwinding all arguments and their justifications, you can obtain an <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_qmqLxvtsPzZ2s6mpY">ideal philosophy student of perfect emptiness</olink>, to be convinced by a line of reasoning that begins from absolutely no assumptions.</para>
<para>But who is this ideal philosopher of perfect emptiness?  Why, it is just the irreducible core of the ghost!</para>
<para>And that is why (I went on to say) the result of trying to remove all assumptions from a mind, and unwind to the perfect absence of any prior, is not an ideal philosopher of perfect emptiness, but a rock.  What is left of a mind after you remove the source code?  Not the ghost who looks over the source code, but simply... no ghost.</para>
<para>So—and I shall take up this theme again later—wherever you are to locate your notions of <emphasis>validity</emphasis> or <emphasis>worth </emphasis>or <emphasis>rationality</emphasis> or <emphasis>justification</emphasis> or even <emphasis>objectivity,</emphasis> it cannot rely on an argument that is <emphasis>universally compelling to all physically possible minds.</emphasis></para>
<para>Nor can you ground validity in a sequence of justifications that, beginning from nothing, persuades a perfect emptiness.</para>
<para>Oh, there might be argument sequences that would compel any neurologically intact <emphasis>human</emphasis>—like the argument I use to make people <link xl:href="https://yudkowsky.net/singularity/aibox/">let the AI out of the box</link><superscript>1</superscript>—but that is hardly the same thing from a philosophical perspective.</para>
<para>The first great failure of those who try to consider Friendly AI, is the One Great Moral Principle That Is All We Need To Program—aka the <olink targetdoc="item_T3CjiQq6bBgFuzvp6" targetptr="item_NnohDYHNnKDtbiMyp">fake utility function</olink>—and of this I have already spoken.</para>
<para>But the even worse failure is the One Great Moral Principle We Don&apos;t Even <emphasis>Need</emphasis> To Program Because Any AI Must Inevitably Conclude It.  This notion exerts a terrifying unhealthy fascination on those who spontaneously reinvent it; they dream of commands that no sufficiently advanced mind can disobey.  The gods themselves will proclaim the rightness of their philosophy!  (E.g. John C. Wright, Marc Geddes.)</para>
<para>There is also a less severe version of the failure, where the one does not <emphasis>declare </emphasis>the One True Morality.  Rather the one hopes for an AI created <emphasis>perfectly free</emphasis>, unconstrained by flawed humans desiring slaves, so that the AI may arrive at virtue of its own accord—virtue undreamed-of perhaps by the speaker, who confesses themselves too flawed to teach an AI.  (E.g. John K Clark, Richard Hollerith?, <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_DwtYPRuCxpXTrzG9m">Eliezer</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_DwtYPRuCxpXTrzG9m"><subscript>1996</subscript></olink>.) This is a less tainted motive than the dream of absolute command. But though <emphasis>this</emphasis> dream arises from virtue rather than vice, it is still based on a flawed understanding of <link xl:href="https://www.lesswrong.com/posts/EsMhFZuycZorZNRF5/the-ultimate-source">freedom</link>, and will not actually <emphasis>work in real life.</emphasis>  Of this, more to follow, of course.</para>
<para>John C. Wright, who was previously writing a very nice transhumanist trilogy (first book:<emphasis> The Golden Age</emphasis>) inserted a huge Author Filibuster in the middle of his climactic third book, describing in tens of pages his Universal Morality That Must Persuade Any AI.  I don&apos;t know if anything happened after that, because I stopped reading.  And then Wright converted to Christianity—yes, seriously.  So you <emphasis>really don&apos;t</emphasis> want to fall into this trap!</para>
<para>Footnote 1: Just kidding.</para>

  
</section>
