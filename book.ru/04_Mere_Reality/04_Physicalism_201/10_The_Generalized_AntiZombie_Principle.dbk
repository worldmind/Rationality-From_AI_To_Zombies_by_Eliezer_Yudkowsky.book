<section xml:id="item_kYAuNJX2ecH2uFqZ9"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>The Generalized Anti-Zombie Principle</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/kYAuNJX2ecH2uFqZ9/the-generalized-anti-zombie-principle"></link>
    </bibliosource>
    <pubdate doc_status="draft">2008-04-06</pubdate>
  </info>
  <indexterm><primary>Consciousness</primary></indexterm>
<indexterm><primary>Causality</primary></indexterm>
<indexterm><primary>Zombies</primary></indexterm>
  <blockquote>
  <para> &quot;Each problem that I solved became a rule which served afterwards to solve other problems.&quot;</para>
  <para>        —Rene Descartes, <emphasis>Discours de la Methode</emphasis></para>
</blockquote>
<para>&quot;<olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_fdEWWr8St59bXLbQr">Zombies</olink>&quot; are putatively beings that are atom-by-atom identical to us, governed by all the same third-party-visible physical laws, except that they are not conscious.</para>
<para>Though the philosophy is complicated, the <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_fdEWWr8St59bXLbQr">core argument against zombies</olink> is simple:  When you focus your inward awareness on your inward awareness, soon after your internal narrative (the little voice inside your head that speaks your thoughts) says &quot;I am aware of being aware&quot;, and then you say it out loud, and then you type it into a computer keyboard, and create a third-party visible blog post.</para>
<para>Consciousness, whatever it may be—a substance, a process, a name for a confusion—is not epiphenomenal; your mind can catch the inner listener in the act of listening, and say so out loud.  <emphasis>The fact that I have typed this paragraph</emphasis> would at least <emphasis>seem</emphasis> to refute the idea that consciousness has no experimentally detectable consequences.</para>
<para>I hate to say &quot;So now let&apos;s accept this and move on,&quot; over such a philosophically controversial question, but it seems like a considerable majority of Overcoming Bias commenters do accept this.  And there are other conclusions you can only get to after you accept that you cannot subtract consciousness and leave the universe looking exactly the same.  So now let&apos;s accept this and move on.</para>
<para>The form of the Anti-Zombie Argument seems like it should generalize, becoming an Anti-Zombie Principle.  But what is the proper generalization?</para>
<para>Let&apos;s say, for example, that someone says:  &quot;I have a switch in my hand, which does not affect your brain in any way; and iff this switch is flipped, you will cease to be conscious.&quot;  Does the Anti-Zombie Principle rule this out as well, with the same structure of argument?</para>
<para>It appears to me that in the case above, the answer is yes.  In particular, you can say:  &quot;Even after your switch is flipped, I will still talk about consciousness <emphasis>for exactly the same reasons</emphasis> I did before.  If I am conscious right now, I will still be conscious after you flip the switch.&quot;</para>
<para>Philosophers may object, &quot;But now you&apos;re equating consciousness with talking about consciousness!  What about the Zombie Master, the chatbot that regurgitates a remixed corpus of amateur human discourse on consciousness?&quot;</para>
<para>But I did <emphasis>not</emphasis> equate &quot;consciousness&quot; with verbal behavior.  The core premise is that, <emphasis>among other things,</emphasis> the <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_gRa5cWWBsZqdFvmqu">true referent</olink> of &quot;consciousness&quot; is <emphasis>also</emphasis> the <emphasis>cause in humans</emphasis> of talking about inner listeners.</para>
<para>As I argued (at some length) in the <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_FaJaCgqBKphrDzDSj">sequence on words</olink>, what you want in defining a word is not always a perfect <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_bcM5ft8jvsffsZZ4Y">Aristotelian</olink> necessary-and-sufficient definition; sometimes you just want a <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_HsznWM9A7NiuGsp28">treasure map</olink> that leads you to the extensional referent.  So &quot;that which <emphasis>does in fact</emphasis> make me talk about an unspeakable awareness&quot; is not a necessary-and-sufficient definition.  But if what does <emphasis>in fact</emphasis> cause me to discourse about an unspeakable awareness, is not &quot;consciousness&quot;, then...</para>
<para>...then the discourse gets pretty futile.  That is not a knockdown argument against zombies—an <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_hQxYBfu2LPc9Ydo6w">empirical</olink> question can&apos;t be settled by mere difficulties of discourse.  But if you try to defy the Anti-Zombie Principle, you will have problems with the <emphasis>meaning</emphasis> of your discourse, not just its plausibility.</para>
<para>Could we <emphasis>define</emphasis> the word &quot;consciousness&quot; to mean &quot;whatever actually makes humans talk about &apos;consciousness&apos;&quot;?  This would have the powerful advantage of guaranteeing that there is at least one real fact named by the word &quot;consciousness&quot;.  Even if our belief in consciousness is a confusion, &quot;consciousness&quot; would name the cognitive architecture that generated the confusion.  But to establish a definition is only to promise to use a word consistently; it doesn&apos;t settle any empirical questions, such as whether our inner awareness makes us talk about our inner awareness.</para>
<para>Let&apos;s return to the Off-Switch.</para>
<para>If we allow that the Anti-Zombie Argument applies against the Off-Switch, then the Generalized Anti-Zombie Principle does <emphasis>not</emphasis> say only, &quot;Any change that is not in-principle experimentally detectable (IPED) cannot remove your consciousness.&quot;  The switch&apos;s flipping is experimentally detectable, but it still seems <emphasis>highly </emphasis>unlikely to remove your consciousness.</para>
<para>Perhaps the Anti-Zombie Principle says, &quot;Any change that does not affect you in any IPED way cannot remove your consciousness&quot;?</para>
<para>But is it a reasonable stipulation to say that flipping the switch does not affect you in <emphasis>any</emphasis> IPED way?  All the particles in the switch are interacting with the particles composing your body and brain.  There are gravitational effects—tiny, but real and IPED.  The gravitational pull from a one-gram switch ten meters away is <link xl:href="http://www.google.com/search?hl=en&amp;safe=off&amp;q=G+*+1+gram+%2F+%2810+meters%29%5E2&amp;btnG=Search">around</link> 6 * 10<superscript>-16</superscript> m/s<superscript>2</superscript>.  That&apos;s around half a neutron diameter per second per second, far below thermal noise, but way above the Planck level.</para>
<para>We could flip the switch light-years away, in which case the flip would have no immediate causal effect on you (whatever &quot;immediate&quot; means in this case) (if the Standard Model of physics is correct).</para>
<para>But it doesn&apos;t seem like we <emphasis>should</emphasis> have to alter the thought experiment in this fashion.  It seems that, if a disconnected switch is flipped on the other side of a room, you should not expect your inner listener to go out like a light, because the switch &quot;obviously doesn&apos;t change&quot; that which is the true cause of your talking about an inner listener.  Whatever you really are, you don&apos;t expect the switch to mess with it.</para>
<para>This is a <emphasis>large</emphasis> step.</para>
<para>If you deny that it is a reasonable step, you had better never go near a switch again.  But still, it&apos;s a large step.</para>
<para>The key idea of <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_tPqQdLCuxanjhoaNs">reductionism</olink> is that our maps of the universe are multi-level to save on computing power, but physics seems to be strictly single-level.  All our discourse about the universe takes place using <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_gRa5cWWBsZqdFvmqu">references far above</olink> the level of fundamental particles.</para>
<para>The switch&apos;s flip <emphasis>does</emphasis> change the fundamental particles of your body and brain.  It nudges them by whole neutron diameters away from where they would have otherwise been.</para>
<para>In ordinary life, we gloss a change this small by saying that the switch &quot;doesn&apos;t affect you&quot;.  But it <emphasis>does</emphasis> affect you.  It changes everything by whole neutron diameters!  What could possibly be remaining the same?  Only the <emphasis>description</emphasis> that you would give of the higher levels of organization—the cells, the proteins, the spikes traveling along a neural axon.  As the map is far less detailed than the territory, it must map <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_y5MxoeacRKKM3KQth">many different states to the same description</olink>.</para>
<para>Any reasonable sort of humanish <emphasis>description</emphasis> of the brain that talks about neurons and activity patterns (or even the conformations of individual microtubules making up axons and dendrites) won&apos;t change when you flip a switch on the other side of the room.  Nuclei are larger than neutrons, atoms are larger than nuclei, and by the time you get up to talking about the <emphasis>molecular</emphasis> level, that tiny little gravitational force has vanished from the list of things you bother to <emphasis>track</emphasis>.</para>
<para>But if you add up enough tiny little gravitational pulls, they will eventually yank you across the room and tear you apart by tidal forces, so clearly a small effect is <emphasis>not</emphasis>&quot;no effect at all&quot;.</para>
<para>Maybe the tidal force from that tiny little pull, by an <emphasis>amazing</emphasis> coincidence, pulls a single extra calcium ion just a tiny bit closer to an ion channel, causing it to be pulled in just a tiny bit sooner, making a single neuron fire infinitesimally sooner than it would otherwise have done, a difference which amplifies chaotically, finally making a whole neural spike occur that otherwise wouldn&apos;t have occurred, sending you off on a different train of thought, that triggers an epileptic fit, that kills you, causing you to cease to be conscious...</para>
<para>If you add up a lot of tiny quantitative effects, you get a big quantitative effect—big enough to mess with anything you care to name.  And so claiming that the switch has literally <emphasis>zero</emphasis> effect on the things you care about, is taking it too far.</para>
<para>But with just one switch, the force exerted is vastly less than thermal uncertainties, never mind quantum uncertainties.  If you don&apos;t expect your consciousness to flicker in and out of existence as the result of thermal jiggling, then you <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_PmQkensvTGg7nGtJE">certainly</olink> shouldn&apos;t expect to go out like a light when someone sneezes a kilometer away.</para>
<para>The alert Bayesian will note that I have just made an argument about <emphasis>expectations,</emphasis> states of <emphasis>knowledge,</emphasis> justified <emphasis>beliefs</emphasis> about what can and can&apos;t switch off your consciousness.</para>
<para>This doesn&apos;t necessarily destroy the Anti-Zombie Argument.  <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_zFuCxbY9E2E8HTbfZ">Probabilities are not certainties, but the <emphasis>laws of</emphasis> probability are theorems</olink>; if rationality says you can&apos;t believe something on your current information, then that is a law, not a suggestion.</para>
<para>Still, this version of the Anti-Zombie Argument is weaker.  It doesn&apos;t have the nice, clean, absolutely clear-cut status of, &quot;You can&apos;t possibly eliminate consciousness while leaving all the atoms in <emphasis>exactly</emphasis> the same place.&quot;  (Or for &quot;all the atoms&quot; substitute &quot;all causes with in-principle experimentally detectable effects&quot;, and &quot;same wavefunction&quot; for &quot;same place&quot;, etc.)</para>
<para>But the new version of the Anti-Zombie Argument still carries.  You can say, &quot;I don&apos;t know what consciousness really is, and I suspect I may be fundamentally confused about the question.  But if the word refers to anything at all, it refers to something that is, among other things, the cause of my talking about consciousness.  Now, I don&apos;t know why I talk about consciousness.  But it happens inside my skull, and I expect it has something to do with neurons firing.  Or maybe, if I really understood consciousness, I would have to talk about an even more fundamental level than that, like microtubules, or neurotransmitters diffusing across a synaptic channel.  But still, that switch you just flipped has an effect on my neurotransmitters and microtubules that&apos;s much, much less than thermal noise at 310 Kelvin.  So whatever the true cause of my talking about consciousness may be, I don&apos;t expect it to be hugely affected by the gravitational pull from that switch.  Maybe it&apos;s just a tiny little infinitesimal bit affected?  But it&apos;s certainly not going to go out like a light.  I expect to go on talking about consciousness in <emphasis>almost exactly</emphasis> the same way afterward, for <emphasis>almost exactly</emphasis> the same reasons.&quot;</para>
<para>This application of the Anti-Zombie Principle is weaker.  But it&apos;s also much more general.  And, in terms of sheer common sense, correct.</para>
<para>The reductionist and the substance dualist actually have two different versions of the above statement.  The reductionist furthermore says, &quot;Whatever makes me talk about consciousness, it seems likely that the important parts take place on a much higher functional level than atomic nuclei.  Someone who understood consciousness could abstract away from individual neurons firing, and talk about high-level cognitive architectures, and still describe how my mind produces thoughts like &apos;I think therefore I am&apos;.  So nudging things around by the diameter of a nucleon, shouldn&apos;t affect my consciousness (except maybe with very small probability, or by a very tiny amount, or not until after a significant delay).&quot;</para>
<para>The substance dualist furthermore says, &quot;Whatever makes me talk about consciousness, it&apos;s got to be something beyond the computational physics we know, which means that it might very well involve quantum effects.  But still, my consciousness doesn&apos;t flicker on and off whenever someone sneezes a kilometer away.  If it did, I would <emphasis>notice</emphasis>.  It would be like skipping a few seconds, or coming out of a general anesthetic, or sometimes saying, &quot;I don&apos;t think therefore I&apos;m not.&quot;  So since it&apos;s a physical fact that thermal vibrations don&apos;t disturb the stuff of my awareness, I don&apos;t expect flipping the switch to disturb it either.&quot;</para>
<para>Either way, you <emphasis>shouldn&apos;t</emphasis> expect your sense of awareness to vanish when someone says the word &quot;Abracadabra&quot;, even if that does have some infinitesimal physical effect on your brain—</para>
<para>But hold on!  If you <emphasis>hear</emphasis> someone say the word &quot;Abracadabra&quot;, that has a very noticeable effect on your brain—so large, even your brain can notice it.  It may alter your internal narrative; you may think, &quot;Why did that person just say &apos;Abracadabra&apos;?&quot;</para>
<para>Well, but <emphasis>still</emphasis> you expect to go on talking about consciousness in almost exactly the same way afterward, for almost exactly the same reasons.</para>
<para>And again, it&apos;s not that &quot;consciousness&quot; is being <emphasis>equated </emphasis>to &quot;that which makes you talk about consciousness&quot;.  It&apos;s just that consciousness, <emphasis>among other things,</emphasis> makes you talk about consciousness.  So anything that makes your consciousness go out like a light, should make you stop talking about consciousness.</para>
<para>If we do something to you, where you don&apos;t see how it could <emphasis>possibly</emphasis> change your internal narrative—the little voice in your head that sometimes says things like &quot;I think therefore I am&quot;, whose words you can choose to say aloud—then it shouldn&apos;t make you cease to be conscious.</para>
<para>And this is true even if the internal narrative is just &quot;pretty much the same&quot;, and the causes of it are also pretty much the same; among the causes that are pretty much the same, is whatever you mean by &quot;consciousness&quot;.</para>
<para>If you&apos;re wondering where all this is going, and why it&apos;s important to go to such tremendous lengths to ponder such an obvious-seeming Generalized Anti-Zombie Principle, then consider the following debate:</para>
<para>Albert:  &quot;Suppose I replaced all the neurons in your head with tiny robotic artificial neurons that had the same connections, the same local input-output behavior, and analogous internal state and learning rules.&quot;</para>
<para>Bernice:  &quot;That&apos;s killing me!  There wouldn&apos;t be a conscious being there anymore.&quot;</para>
<para>Charles:  &quot;Well, there&apos;d still be a conscious being there, but it wouldn&apos;t be <emphasis>me.</emphasis>&quot;</para>
<para>Sir Roger Penrose:  &quot;The thought experiment you propose is impossible.  You <emphasis>can&apos;t</emphasis> duplicate the behavior of neurons without tapping into quantum gravity.  That said, there&apos;s not much point in me taking further part in this conversation.&quot;  <emphasis>(Wanders away.)</emphasis></para>
<para>Albert:  &quot;Suppose that the replacement is carried out one neuron at a time, and the swap occurs so fast that it doesn&apos;t make any difference to global processing.&quot;</para>
<para>Bernice:  &quot;How could that possibly be the case?&quot;</para>
<para>Albert:  &quot;The little robot swims up to the neuron, surrounds it, scans it, learns to duplicate it, and then suddenly takes over the behavior, between one spike and the next.  In fact, the imitation is <emphasis>so</emphasis> good, that your outward behavior is just the same as it would be if the brain were left undisturbed.  Maybe not <emphasis>exactly </emphasis>the same, but the causal impact is much less than thermal noise at 310 Kelvin.&quot;</para>
<para>Charles:  &quot;So what?&quot;</para>
<para>Albert:  &quot;So don&apos;t your beliefs violate the Generalized Anti-Zombie Principle?  Whatever just happened, it didn&apos;t change your internal narrative!  You&apos;ll go around talking about consciousness for exactly the same reason as before.&quot;</para>
<para>Bernice:  &quot;Those little robots are a Zombie Master.  They&apos;ll make me talk about consciousness even though I&apos;m not conscious.  The Zombie World is possible if you allow there to be an added, extra, experimentally detectable Zombie Master—which those robots <emphasis>are</emphasis>.&quot;</para>
<para>Charles:  &quot;Oh, that&apos;s not right, Bernice.  The little robots aren&apos;t plotting how to fake consciousness, or processing a corpus of text from human amateurs.  They&apos;re doing the same thing neurons do, just in silicon instead of carbon.&quot;</para>
<para>Albert:  &quot;Wait, didn&apos;t you just agree with me?&quot;</para>
<para>Charles:  &quot;I never said the new person wouldn&apos;t be conscious.  I said it wouldn&apos;t be <emphasis>me.</emphasis>&quot;</para>
<para>Albert:  &quot;Well, obviously the Anti-Zombie Principle generalizes to say that this operation hasn&apos;t disturbed the true cause of your talking about this <emphasis>me</emphasis> thing.&quot;</para>
<para>Charles:  &quot;Uh-uh!  Your operation certainly did disturb the true cause of my talking about consciousness.  It substituted a <emphasis>different</emphasis> cause in its place, the robots.  Now, just because that new cause <emphasis>also</emphasis> happens to be conscious—talks about consciousness for the same <emphasis>generalized</emphasis> reason—doesn&apos;t mean it&apos;s the <emphasis>same</emphasis> cause that was originally there.&quot;</para>
<para>Albert:  &quot;But I wouldn&apos;t even have to <emphasis>tell</emphasis> you about the robot operation.  You wouldn&apos;t <emphasis>notice.</emphasis>  If you think, going on introspective evidence, that you are in an important sense &quot;the same person&quot; that you were five minutes ago, and I do something to you that doesn&apos;t change the introspective evidence available to you, then your conclusion that you are the same person that you were five minutes ago should be equally justified.  Doesn&apos;t the Generalized Anti-Zombie Principle say that if I do something to you that alters your consciousness, let alone makes you a completely different person, then you ought to <emphasis>notice</emphasis> somehow?&quot;</para>
<para>Bernice:  &quot;Not if you replace me with a Zombie Master.  Then there&apos;s no one there <emphasis>to</emphasis> notice.&quot;</para>
<para>Charles:  &quot;Introspection isn&apos;t perfect.  Lots of stuff goes on inside my brain that I don&apos;t notice.&quot;</para>
<para>Albert:  &quot;You&apos;re postulating epiphenomenal facts about consciousness and identity!&quot;</para>
<para>Bernice:  &quot;No I&apos;m not!  I can experimentally detect the difference between neurons and robots.&quot;</para>
<para>Charles:  &quot;No I&apos;m not!  I can experimentally detect the moment when the old me is replaced by a new person.&quot;</para>
<para>Albert:  &quot;Yeah, and I can detect the switch flipping!  You&apos;re detecting something that doesn&apos;t <emphasis>make a noticeable difference</emphasis> to the<emphasis> true cause</emphasis> of your talk about consciousness and personal identity.  And the proof is, you&apos;ll talk just the same way afterward.&quot;</para>
<para>Bernice:  &quot;That&apos;s because of your robotic Zombie Master!&quot;</para>
<para>Charles:  &quot;Just because two people talk about &apos;personal identity&apos; for similar reasons doesn&apos;t make them the same person.&quot;</para>
<para>I think the Generalized Anti-Zombie Principle supports Albert&apos;s position, but the reasons shall have to wait for future posts.  I need other prerequisites, and besides, this post is already too long.</para>
<para>But you see the importance of the question, &quot;How far can you generalize the <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_fdEWWr8St59bXLbQr">Anti-Zombie Argument</olink> and have it still be valid?&quot;</para>
<para>The makeup of future galactic civilizations may be determined by the answer...</para>

  
</section>
