<section xml:id="item_synsRtBKDeAFuo7e3"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>Not for the Sake of Happiness (Alone)</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/synsRtBKDeAFuo7e3/not-for-the-sake-of-happiness-alone"></link>
    </bibliosource>
    <pubdate doc_status="draft">2007-11-22</pubdate>
  </info>
  <indexterm><primary>Happiness</primary></indexterm>
<indexterm><primary>Well-being</primary></indexterm>
<indexterm><primary>Fuzzies</primary></indexterm>
  <para>When I met the futurist Greg Stock some years ago, he argued that the joy of scientific discovery would soon be replaced by pills that could simulate the joy of scientific discovery.  I approached him after his talk and said, &quot;I agree that such pills are probably possible, but I wouldn&apos;t voluntarily <emphasis>take them.</emphasis>&quot;</para>
<para>And Stock said, &quot;But they&apos;ll be so much better that <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_Jq73GozjsuhdwMLEG">the real thing won&apos;t be able to compete</olink>.  It will just be way more fun for you to take the pills than to do all the actual scientific work.&quot;</para>
<para>And I said, &quot;I <emphasis>agree</emphasis> that&apos;s possible, so I&apos;ll make sure never to take them.&quot;</para>
<para>Stock seemed genuinely surprised by my attitude, which genuinely surprised <emphasis>me.</emphasis></para>
<para>One often sees ethicists arguing as if all human desires are reducible, in principle, to the desire for ourselves and others to be happy.  (In particular, Sam Harris does this in <emphasis>The End of Faith,</emphasis> which I just finished perusing - though Harris&apos;s reduction is more of a drive-by shooting than a major topic of discussion.)</para>
<para>This isn&apos;t the same as arguing whether <link xl:href="https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks">all happinesses can be measured on a common utility scale</link> - different happinesses might occupy different scales, or be otherwise non-convertible.  And it&apos;s not the same as <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_n5ucT5ZbPdhfGNLtP">arguing that it&apos;s theoretically impossible to value anything other than your own psychological states</olink>, because it&apos;s still permissible to care whether <emphasis>other</emphasis> people are happy.</para>
<para>The question, rather, is whether we <emphasis>should</emphasis> care about the things that <emphasis>make</emphasis> us happy, apart from any happiness they bring.</para>
<para>We can easily list many cases of moralists going astray by caring about things besides happiness.  The various states and countries that still outlaw oral sex make a good example; these legislators would have been better off if they&apos;d said, &quot;Hey, whatever turns you on.&quot;  But this doesn&apos;t show that <emphasis>all</emphasis> values are reducible to happiness; it just argues that in <emphasis>this particular</emphasis><emphasis>case </emphasis>it was an ethical mistake to focus on anything else.</para>
<para>It is an undeniable fact that we tend to do things that make us happy, but this doesn&apos;t mean we should regard the happiness as the <emphasis>only</emphasis> reason for so acting.  First, this would make it difficult to explain how we could care about anyone else&apos;s happiness - how we could treat people as ends in themselves, rather than instrumental means of obtaining a warm glow of satisfaction.</para>
<para>Second, just because something is a consequence of my action doesn&apos;t mean it was the sole justification.  If I&apos;m writing a blog post, and I get a headache, I may take an ibuprofen.  <emphasis>One</emphasis> of the consequences of my action is that I experience less pain, but this doesn&apos;t mean it was the <emphasis>only</emphasis> consequence, or even the most important reason for my decision.  I do value the state of not having a headache.  But I can value something for its own sake <emphasis>and also</emphasis> value it as a means to an end.</para>
<para>For all value to be reducible to happiness, it&apos;s not enough to show that happiness is involved in most of our decisions - it&apos;s not even enough to show that happiness is the <emphasis>most</emphasis> important consequent in <emphasis>all</emphasis> of our decisions - it must be the <emphasis>only</emphasis> consequent.  That&apos;s a tough standard to meet.  (I originally found this point in a Sober and Wilson paper, not sure which one.)</para>
<para>If I claim to value art for its own sake, then would I value art that no one ever saw?  A screensaver running in a closed room, producing beautiful pictures that no one ever saw?  I&apos;d have to say no.  I can&apos;t think of any completely lifeless object that I would value as an end, not just a means.  That would be like valuing ice cream as an end in itself, apart from anyone eating it.  Everything I value, that I can think of, involves people and their experiences <emphasis>somewhere</emphasis> along the line.</para>
<para>The best way I can put it, is that my moral intuition appears to require <emphasis>both</emphasis> the objective and subjective component to grant full value.</para>
<para>The value of scientific discovery requires <emphasis>both</emphasis> a genuine scientific discovery, and a person to take joy in that discovery.  It may seem difficult to disentangle these values, but the pills make it clearer.</para>
<para>I would be disturbed if people retreated into holodecks and fell in love with mindless wallpaper.  I would be disturbed <emphasis>even if they weren&apos;t aware it was a holodeck</emphasis>, which is an important ethical issue if some agents can potentially transport people into holodecks and substitute zombies for their loved ones without their awareness.  Again, the pills make it clearer:  I&apos;m not just concerned with my own awareness of the uncomfortable fact.  I wouldn&apos;t put myself into a holodeck even if I could take a pill to forget the fact afterward.  That&apos;s simply not where I&apos;m trying to steer the future.</para>
<para>I value freedom:  When I&apos;m deciding where to steer the future, I take into account not only the subjective states that people end up in, but also whether they got there as a result of their own efforts.  The presence or absence of an external puppet master can affect my valuation of an otherwise fixed outcome.  Even if people wouldn&apos;t know they were being manipulated, it would matter to my judgment of how well humanity had done with its future.  This is an important ethical issue, if you&apos;re dealing with agents powerful enough to helpfully tweak people&apos;s futures without their knowledge.</para>
<para>So my values are not strictly reducible to happiness:  There are properties I value about the future that aren&apos;t reducible to activation levels in anyone&apos;s pleasure center; properties that are not <emphasis>strictly</emphasis> reducible to subjective states even in principle.</para>
<para>Which means that my decision system has a <emphasis>lot</emphasis> of <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_n5ucT5ZbPdhfGNLtP">terminal values</olink>, none of them strictly reducible to <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_cSXZpvqpa9vbGGLtG">anything else</olink>.  Art, science, love, lust, freedom, friendship...</para>
<para>And I&apos;m okay with that.  I value a life complicated enough to be challenging and aesthetic - not just the <emphasis>feeling</emphasis> that life is complicated, but the <emphasis>actual</emphasis> complications - so turning into a pleasure center in a vat doesn&apos;t appeal to me.  It would be a waste of humanity&apos;s potential, which I value actually fulfilling, not just having the feeling that it was fulfilled.</para>

  
</section>
