<section xml:id="item_BA7dRRrzMLyvfJr9J"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>My Best and Worst Mistake</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/BA7dRRrzMLyvfJr9J/my-best-and-worst-mistake"></link>
    </bibliosource>
    <pubdate doc_status="draft">2008-09-16</pubdate>
  </info>
  <indexterm><primary>General Intelligence</primary></indexterm>
<indexterm><primary>Growth Stories</primary></indexterm>
  <para>Yesterday I covered the young Eliezer&apos;s affective death spiral around something that he called &quot;intelligence&quot;.  Eliezer<subscript>1996</subscript>, or even Eliezer<subscript>1999</subscript> for that matter, would have refused to try and put a mathematical definition—consciously, deliberately refused.  Indeed, he would have been loath to put any definition on &quot;intelligence&quot; at all.</para>
<para>Why?  Because there&apos;s a standard bait-and-switch problem in AI, wherein you define &quot;intelligence&quot; to mean something like &quot;logical reasoning&quot; or &quot;the ability to withdraw conclusions when they are no longer appropriate&quot;, and then you build a cheap theorem-prover or an ad-hoc nonmonotonic reasoner, and then say, &quot;Lo, I have implemented intelligence!&quot;  People came up with poor definitions of intelligence—focusing on correlates rather than cores—and then they chased the surface definition they had written down, forgetting about, you know, actual <emphasis>intelligence.</emphasis>  It&apos;s not like Eliezer<subscript>1996</subscript> was out to build a career in Artificial Intelligence.  He just wanted a mind that would actually be able to build nanotechnology.  So he wasn&apos;t tempted to redefine intelligence for the sake of puffing up a paper.</para>
<para>Looking back, it seems to me that quite a lot of my mistakes can be defined in terms of being <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_qNZM3EGoE5ZeMdCRt">pushed too far in the other direction</olink> by seeing someone else stupidity:  Having seen attempts to define &quot;intelligence&quot; abused so often, I refused to define it at all.  What if I said that intelligence was X, and it wasn&apos;t <emphasis>really</emphasis> X?  I knew in an intuitive sense what I was looking for—something powerful enough to take stars apart for raw material—and I didn&apos;t want to fall into the trap of being distracted from that by definitions.</para>
<para>Similarly, having seen so many AI projects brought down by physics envy—trying to stick with simple and elegant math, and being constrained to toy systems as a result—I generalized that any math simple enough to be formalized in a neat equation was probably not going to work for, you know, <emphasis>real</emphasis> intelligence.  &quot;Except for Bayes&apos;s Theorem,&quot; Eliezer<subscript>2000</subscript> added; which, depending on your viewpoint, either mitigates the totality of his offense, or shows that he should have suspected the entire generalization instead of trying to add a single exception.</para>
<para>If you&apos;re wondering why Eliezer<subscript>2000</subscript> thought such a thing—disbelieved in a math of intelligence—well, it&apos;s hard for me to remember this far back.  It certainly wasn&apos;t that I ever disliked math.  If I had to point out a root cause, it would be reading too few, too popular, and the wrong Artificial Intelligence books.</para>
<para>But then I didn&apos;t think the answers were going to come from Artificial Intelligence; I had mostly written it off as a sick, dead field.  So it&apos;s no wonder that I spent too little time investigating it.  I believed in the cliche about Artificial Intelligence overpromising.  You can fit that into the pattern of &quot;too far in the opposite direction&quot;—the field hadn&apos;t delivered on its promises, so I was ready to write it off.  As a result, I didn&apos;t investigate hard enough to find the math that wasn&apos;t fake.</para>
<para>My youthful disbelief in a mathematics of general intelligence was simultaneously one of my all-time worst mistakes, and one of my all-time best mistakes.</para>
<para>Because I disbelieved that there could be any simple answers to intelligence, I went and I read up on cognitive psychology, functional neuroanatomy, computational neuroanatomy, evolutionary psychology, evolutionary biology, and more than one branch of Artificial Intelligence.  When I had what seemed like simple bright ideas, I didn&apos;t stop there, or rush off to try and implement them, because I knew that even if they were true, even if they were necessary, they wouldn&apos;t be sufficient: intelligence wasn&apos;t supposed to be simple, it wasn&apos;t supposed to have an answer that fit on a T-Shirt.  It was supposed to be a big puzzle with lots of pieces; and when you found one piece, you didn&apos;t run off holding it high in triumph, you kept on looking.  Try to build a mind with a single missing piece, and it might be that nothing interesting would happen.</para>
<para>I was wrong in thinking that Artificial Intelligence the academic field, was a desolate wasteland; and even wronger in thinking that there couldn&apos;t be math of intelligence.  But I don&apos;t regret studying e.g. functional neuroanatomy, even though I <emphasis>now</emphasis> think that an Artificial Intelligence should look nothing like a human brain.  Studying neuroanatomy meant that I went in with the idea that if you broke up a mind into pieces, the pieces were things like &quot;visual cortex&quot; and &quot;cerebellum&quot;—rather than &quot;stock-market trading module&quot; or &quot;commonsense reasoning module&quot;, which is a standard wrong road in AI.</para>
<para>Studying fields like functional neuroanatomy and cognitive psychology gave me a very different idea of what minds had to look like, than you would get from just reading AI books—even good AI books.</para>
<para>When you blank out all the wrong conclusions and wrong justifications, and just ask what that belief led the young Eliezer to actually <emphasis>do...</emphasis></para>
<para>Then the belief that Artificial Intelligence was sick and that the real answer would have to come from healthier fields outside, led him to study lots of cognitive sciences;</para>
<para>The belief that AI couldn&apos;t have simple answers, led him to not stop prematurely on one brilliant idea, and to accumulate lots of information;</para>
<para>The belief that you didn&apos;t want to define intelligence, led to a situation in which he <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_uHYYA32CKgKT3FagE">studied the problem for a long time</olink> before, years later, he started to propose systematizations.</para>
<para>This is what I refer to when I say that this is one of my all-time best mistakes.</para>
<para>Looking back, years afterward, I drew a very strong moral, to this effect:</para>
<para>What you actually end up doing, screens off the clever reason why you&apos;re doing it.</para>
<para>Contrast amazing clever reasoning that leads you to study many sciences, to amazing clever reasoning that says you don&apos;t need to read all those books.  Afterward, when your amazing clever reasoning turns out to have been stupid, you&apos;ll have ended up in a much better position, if your amazing clever reasoning was of the first type.</para>
<para>When I look back upon my past, I am struck by the number of semi-accidental successes, the number of times I did something right for the wrong reason.  From your perspective, you should chalk this up to the anthropic principle: if I&apos;d fallen into a true dead end, you probably wouldn&apos;t be hearing from me on this blog.  From my perspective it remains something of an embarrassment.  My <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_DwtYPRuCxpXTrzG9m">Traditional Rationalist</olink> upbringing provided a lot of directional bias to those &quot;accidental successes&quot;—biased me toward rationalizing reasons to study rather than not study, prevented me from getting completely lost, helped me recover from mistakes.  Still, none of that was the right action for the right reason, and that&apos;s a scary thing to look back on your youthful history and see.  One of my primary purposes in writing on <emphasis>Overcoming Bias</emphasis> is to leave a trail to where I ended up by accident—to obviate the role that luck played in my own forging as a rationalist.</para>
<para>So what makes this one of my all-time worst mistakes?  Because sometimes &quot;informal&quot; is another way of saying &quot;held to low standards&quot;. I had amazing clever reasons why it was okay for me not to precisely define &quot;intelligence&quot;, and certain of my other terms as well: namely,other people had gone astray by trying to define it.  This was a gate through which sloppy reasoning could enter.</para>
<para>So should I have jumped ahead and tried to forge an exact definition right away?  No, all the reasons why I knew this was the wrong thing to do, were correct; you can&apos;t conjure the right definition out of thin air if your knowledge is not adequate.</para>
<para>You can&apos;t get to <emphasis>the</emphasis> definition of fire if you don&apos;t know about atoms and molecules; you&apos;re better off saying &quot;that orangey-bright thing&quot;.  And you do have to be able to talk about that orangey-bright stuff, even if you can&apos;t say exactly what it is, to investigate fire.  But these days I would say that all reasoning on that level is something that can&apos;t be trusted—rather it&apos;s something you do on the way to knowing better, but you don&apos;t <emphasis>trust</emphasis> it, you don&apos;t <emphasis>put your weight down</emphasis> on it, you don&apos;t draw firm conclusions from it, no matter how inescapable the informal reasoning seems.</para>
<para>The young Eliezer put his weight down on the wrong floor tile—stepped onto a loaded trap.  To be continued.</para>

  
</section>
