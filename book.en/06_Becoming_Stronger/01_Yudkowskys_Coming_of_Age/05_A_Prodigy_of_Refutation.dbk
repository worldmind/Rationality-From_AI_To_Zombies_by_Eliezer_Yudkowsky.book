<section xml:id="item_CcBe9aCKDgT5FSoty"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>A Prodigy of Refutation</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/CcBe9aCKDgT5FSoty/a-prodigy-of-refutation"></link>
    </bibliosource>
    <pubdate doc_status="draft">2008-09-18</pubdate>
  </info>
  <indexterm><primary>Steelmanning</primary></indexterm>
<indexterm><primary>Growth Stories</primary></indexterm>
  <para><olink targetdoc="item_e6WsPsivzBifrWHeA" targetptr="item_uD9TDHPwQ5hx4CgaX">My Childhood Death Spiral</olink> described the core momentum carrying me into my mistake, an <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_XrzQW69HpidzvBxGr">affective death spiral</olink> around something that Eliezer<subscript>1996</subscript> called &quot;intelligence&quot;.  I was also a <olink targetdoc="item_e6WsPsivzBifrWHeA" targetptr="item_uNWRXtdwL33ELgWjD">technophile</olink>, pre-allergized against fearing the future.  And I&apos;d read a lot of science fiction built around personhood ethics—in which fear of the Alien puts humanity-at-large in the position of the bad guys, mistreating aliens or sentient AIs because they &quot;aren&apos;t human&quot;.</para>
<para>That&apos;s part of the ethos you acquire from science fiction—to define your in-group, your tribe, appropriately broadly.  Hence my email address, sentience@pobox.com.</para>
<para>So Eliezer<subscript>1996</subscript> is out to build superintelligence, for the good of humanity and all sentient life.</para>
<para>At first, I think, the question of whether a superintelligence will/could be good/evil didn&apos;t really occur to me as a separate topic of discussion.  Just the standard intuition of, &quot;Surely no supermind would be stupid enough to turn the galaxy into paperclips; surely, being so intelligent, it will also know what&apos;s <emphasis>right</emphasis> far better than a human being could.&quot;</para>
<para>Until I introduced myself and my quest to a transhumanist mailing list, and got back responses along the general lines of (from memory):</para>
<blockquote>
  <para> Morality is arbitrary—if you say that something is good or bad, you can&apos;t be right or wrong about that.  A superintelligence would form its own morality.</para>
  <para>Everyone ultimately looks after their own self-interest.  A superintelligence would be no different; it would just seize all the resources.</para>
  <para>Personally, I&apos;m a human, so I&apos;m in favor of humans, not Artificial Intelligences.  I don&apos;t think we should develop this technology. Instead we should develop the technology to upload humans first.</para>
  <para>No one should develop an AI without a control system that watches it and makes sure it can&apos;t do anything bad.</para>
</blockquote>
<para>Well, <emphasis>that&apos;s</emphasis> all obviously wrong, thinks Eliezer<subscript>1996</subscript>, and he proceeded to kick his opponents&apos; arguments to pieces.  (I&apos;ve mostly done this in other blog posts, and anything remaining is left as an exercise to the reader.)</para>
<para>It&apos;s not that Eliezer<subscript>1996</subscript>  explicitly reasoned, &quot;The world&apos;s stupidest man says the sun is shining, <emphasis>therefore</emphasis> it is dark out.&quot;  But Eliezer<subscript>1996</subscript> was a Traditional Rationalist; he had been inculcated with the metaphor of science as a <emphasis>fair fight</emphasis> between sides who take on different positions, stripped of mere violence and other such exercises of political muscle, so that, ideally, the side with the best arguments can win.</para>
<para>It&apos;s easier to say where someone else&apos;s argument is wrong, then to get the fact of the matter right; and Eliezer<subscript>1996</subscript> was <emphasis>very skilled</emphasis> at finding flaws.  (So am I.  It&apos;s not as if you can solve <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_AdYdLP2sRqPMoe8fb">the danger of that power</olink> by refusing to care about flaws.)  From Eliezer<subscript>1996</subscript>&apos;s perspective, it seemed to him that his chosen side was <emphasis>winning the fight</emphasis>—that he was formulating better arguments than his opponents—so why would he switch sides?</para>
<para>Therefore is it <link xl:href="http://yudkowsky.net/virtues/">written</link>:  &quot;Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities.  But it is useless to be superior:  Life is not graded on a curve.  The best physicist in ancient Greece could not calculate the path of a falling apple.  There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse.&quot;</para>
<para>You cannot rely on <emphasis>anyone</emphasis> else to argue you out of your mistakes; you cannot rely on <emphasis>anyone</emphasis> else to save you; you and <emphasis>only</emphasis> you are obligated to find the flaws in your positions; if you put that burden down, don&apos;t expect anyone else to pick it up.  And I wonder if that advice will turn out not to help most people, until they&apos;ve personally blown off their own foot, saying to themselves all the while, <emphasis>correctly</emphasis>, &quot;Clearly I&apos;m winning this argument.&quot;</para>
<para>Today I try not to take any human being as my opponent.  That just leads to overconfidence.  It is Nature that I am facing off against, who does not match Her problems to your skill, who is not obliged to offer you a fair chance to win in return for a diligent effort, who does not care if you are the best who ever lived, if you are not good <emphasis>enough.</emphasis></para>
<para>But return to 1996.  Eliezer<subscript>1996</subscript> is going with the basic intuition of &quot;Surely a superintelligence will know better than we could what is <emphasis>right,</emphasis>&quot; and offhandedly knocking down various arguments brought against his position.  He was skillful in that way, you see.  He even had a personal philosophy of why it was wise to look for flaws in things, and so on.</para>
<para>I don&apos;t mean to say it as an excuse, that no one who argued against Eliezer<subscript>1996</subscript>, actually presented him with the <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_Mc6QcrsbH5NRXbCRX">dissolution</olink> of the mystery—the full reduction of morality that analyzes all his cognitive processes debating &quot;morality&quot;, a step-by-step walkthrough of the algorithms that make morality feel to him like a fact.  Consider it rather as an indictment, a measure of Eliezer<subscript>1996</subscript>&apos;s level, that he would have needed the full solution given to him, in order to present him with an argument that he could <emphasis>not</emphasis> refute.</para>
<para>The few philosophers present, did not extract him from his difficulties.  It&apos;s not as if a philosopher will say, &quot;Sorry, morality is understood, it is a settled issue in cognitive science and philosophy, and your viewpoint is simply wrong.&quot;  The nature of morality is still an open question in philosophy, the debate is still going on.  A philosopher will feel obligated to present you with a list of classic arguments on all sides; most of which Eliezer<subscript>1996</subscript> is quite intelligent enough to knock down, and so he concludes that philosophy is a wasteland.</para>
<para>But wait.  It gets worse.</para>
<para>I don&apos;t recall exactly when—it might have been 1997—but the younger me, let&apos;s call him Eliezer<subscript>1997</subscript>, set out to argue<emphasis> inescapably</emphasis> that creating superintelligence is the right thing to do.  To be continued.</para>

  
</section>
