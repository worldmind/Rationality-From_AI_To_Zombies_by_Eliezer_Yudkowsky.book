<section xml:id="item_qmqLxvtsPzZ2s6mpY"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>A Priori</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/qmqLxvtsPzZ2s6mpY/a-priori"></link>
    </bibliosource>
    <pubdate doc_status="draft">2020-03-04</pubdate>
  </info>
  <indexterm><primary>Bayesianism</primary></indexterm>
<indexterm><primary>Philosophy</primary></indexterm>
<indexterm><primary>Truth, Semantics, &amp; Meaning</primary></indexterm>
<indexterm><primary>Occam's Razor</primary></indexterm>
<indexterm><primary>Priors</primary></indexterm>
  <para>Traditional Rationality is phrased as social rules, with violations interpretable as cheating: if you break the rules and no one else is doing so, you&apos;re the first to defect - making you a bad, bad person.  To Bayesians, the brain is an engine of accuracy: <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_eY45uCCX7DdwJ4Jha">if you violate the laws of rationality, the engine doesn&apos;t run</olink>, and this is equally true whether anyone else breaks the rules or not.</para>
<para>Consider the problem of <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_f4txACqDWithRi7hs">Occam&apos;s Razor</olink>, as confronted by Traditional philosophers.  If two hypotheses fit the same observations <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_f4txACqDWithRi7hs">equally well</olink>, why believe the simpler one is more likely to be true?</para>
<para>You could argue that Occam&apos;s Razor has worked in the past, and is therefore likely to continue to work in the future.  But this, itself, appeals to a prediction from Occam&apos;s Razor.  &quot;Occam&apos;s Razor works up to October 8th, 2007 and then stops working thereafter&quot; is more complex, but it fits the observed evidence equally well.</para>
<para>You could argue that Occam&apos;s Razor is a reasonable distribution on prior probabilities.  But what is a &quot;reasonable&quot; distribution?  Why not label &quot;reasonable&quot; a very complicated prior distribution, which makes Occam&apos;s Razor work in all observed tests so far, but generates exceptions in future cases?</para>
<para>Indeed, it seems there is no way to <emphasis>justify</emphasis> Occam&apos;s Razor except by <emphasis>appealing</emphasis> to Occam&apos;s Razor, making this <emphasis>argument</emphasis> unlikely to <emphasis>convince</emphasis> any <emphasis>judge</emphasis> who does not already <emphasis>accept</emphasis> Occam&apos;s Razor.  (What&apos;s special about the words I italicized?)</para>
<para>If you are a philosopher whose daily work is to write papers, criticize other people&apos;s papers, and respond to others&apos; criticisms of your own papers, then you may look at Occam&apos;s Razor and shrug.  Here is an end to justifying, arguing and convincing.  You decide to call a <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_FWMfQKG3RpZx6irjm">truce</olink> on writing papers; if your fellow philosophers do not demand justification for your un-arguable beliefs, you will not demand justification for theirs.  And as the symbol of your treaty, your white flag, you use the phrase &quot;<olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_6FmqiAgS8h4EJm86s">a priori truth</olink>&quot;.</para>
<para>But to a Bayesian, in this era of cognitive science and evolutionary biology and Artificial Intelligence, saying &quot;a priori&quot; doesn&apos;t explain why the brain-engine runs.  If the brain has an amazing &quot;a priori truth factory&quot; that <emphasis>works</emphasis> to produce accurate beliefs, it makes you wonder why a thirsty hunter-gatherer can&apos;t use the &quot;a priori truth factory&quot; to locate drinkable water.  It makes you wonder why eyes evolved in the first place, if there are ways to produce accurate beliefs without <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_6s3xABaXKPdFwA3FS">looking at things</olink>.</para>
<para>James R. Newman said:  &quot;The fact that one apple added to one apple invariably gives two apples helps in the teaching of arithmetic, but has no bearing on the truth of the proposition that 1 + 1 = 2.&quot;  The Internet Encyclopedia of Philosophy <link xl:href="http://www.iep.utm.edu/a/apriori.htm">defines</link> &quot;a priori&quot; propositions as those knowable independently of experience.  Wikipedia <link xl:href="http://en.wikipedia.org/wiki/A_priori_and_a_posteriori_%28philosophy%29">quotes</link> Hume:  Relations of ideas are &quot;discoverable by the mere operation of thought, without dependence on what is anywhere existent in the universe.&quot;  You can see that 1 + 1 = 2 <emphasis>just by thinking about it,</emphasis> without looking at apples.</para>
<para>But in this era of neurology, one ought to be aware that <emphasis>thoughts</emphasis> are existent in the universe; they are identical to the operation of brains.  Material brains, real in the universe, composed of quarks in a single unified mathematical physics whose laws draw no border between the inside and outside of your skull.</para>
<para>When you add 1 + 1 and get 2 by thinking, these thoughts are themselves embodied in flashes of neural patterns.  In principle, we could <emphasis>observe</emphasis>, experientially, the exact same material events as they occurred within someone else&apos;s brain.  It would require some advances in computational neurobiology and brain-computer interfacing, but in principle, it could be done.  You could see someone else&apos;s engine operating materially, through material chains of cause and effect, to compute by &quot;pure thought&quot; that 1 + 1 = 2.  How is observing this pattern in <emphasis>someone else&apos;s</emphasis> brain any different, as a way of knowing, from observing your own brain doing the same thing?  When &quot;pure thought&quot; tells you that 1 + 1 = 2, &quot;independently of any experience or observation&quot;, you are, in effect, observing your own brain as evidence.</para>
<para>If this seems counterintuitive, try to see minds/brains as engines - an engine that collides the neural pattern for 1 and the neural pattern for 1 and gets the neural pattern for 2.  If this engine works at all, then it should <emphasis>have the same output</emphasis> if it observes (with eyes and retina) a similar brain-engine carrying out a similar collision, and copies into itself the resulting pattern.  In other words, for every form of a priori knowledge obtained by &quot;pure thought&quot;, you are learning exactly the same thing you would learn if you saw an outside brain-engine carrying out the same pure flashes of neural activation.  The engines are equivalent, the <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_34XxbRFe54FycoCDw">bottom-line outputs</olink> are equivalent, the <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_6s3xABaXKPdFwA3FS">belief-entanglements</olink> are the same.</para>
<para>There is nothing you can know &quot;a priori&quot;, which you could not know with equal validity by observing the chemical release of neurotransmitters within some outside brain.  What do you think you <emphasis>are,</emphasis> dear reader?</para>
<para>This is <emphasis>why</emphasis> you can predict the result of adding 1 apple and 1 apple by imagining it first in your mind, or punch &quot;3 x 4&quot; into a calculator to predict the result of imagining 4 rows with 3 apples per row.  You and the apple exist within a <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_7iTwGquBFZKttpEdE">boundary-less unified physical process</olink>, and one part may echo another.</para>
<para>Are the sort of neural flashes that philosophers label &quot;a priori beliefs&quot;, <emphasis>arbitrary</emphasis>?  Many AI algorithms function better with &quot;regularization&quot; that biases the solution space toward simpler solutions.  But the regularized algorithms are themselves more complex; they contain an extra line of code (or 1000 extra lines) compared to unregularized algorithms.  The human brain is biased toward simplicity, and we think more efficiently thereby.  If you <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_yxvi9RitzZDpqn6Yh">press the Ignore button</olink> at this point, you&apos;re left with a complex brain that exists for no reason and works for no reason.  So don&apos;t try to tell me that &quot;a priori&quot; beliefs are arbitrary, because they sure aren&apos;t generated by rolling random numbers.  (What does the adjective &quot;arbitrary&quot; <emphasis>mean,</emphasis> anyway?)</para>
<para>You can&apos;t <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_eY45uCCX7DdwJ4Jha">excuse</olink> calling a proposition &quot;a priori&quot; by pointing out that <emphasis>other</emphasis> philosophers are having trouble justifying <emphasis>their</emphasis> propositions.  If a philosopher fails to explain something, this fact cannot supply electricity to a refrigerator, nor act as a magical factory for accurate beliefs.  There&apos;s no truce, no white flag, until you understand why the engine works.</para>
<para>If you clear your mind of <emphasis>justification,</emphasis> of <emphasis>argument,</emphasis> then it seems obvious why Occam&apos;s Razor works in practice: we live in a simple world, a low-entropy universe in which there are short explanations to be found.  &quot;But,&quot; you cry, &quot;why is the universe itself orderly?&quot;  This I do not know, but it is what I see as the next mystery to be <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_yxvi9RitzZDpqn6Yh">explained</olink>.  This is not the same question as &quot;How do I argue Occam&apos;s Razor to a hypothetical debater who has not already accepted it?&quot;</para>
<para>Perhaps you cannot argue <emphasis>anything</emphasis> to a hypothetical debater who has not accepted Occam&apos;s Razor, just as you cannot argue anything to a rock.  A mind needs a certain amount of dynamic structure to be an argument-acceptor.  If a mind doesn&apos;t implement Modus Ponens, it can accept &quot;A&quot; and &quot;A-&gt;B&quot; all day long without ever producing &quot;B&quot;.  How do you justify Modus Ponens to a mind that hasn&apos;t accepted it?  How do you argue a rock into becoming a mind?</para>
<para>Brains evolved from non-brainy matter by natural selection; they were not justified into existence by arguing with an ideal philosophy student of perfect emptiness.  This does not make our judgments meaningless.  A brain-engine can work correctly, producing accurate beliefs, even if it was merely <emphasis>built</emphasis> - by human hands or cumulative stochastic selection pressures - rather than argued into existence.  But to be satisfied by this answer, one must see rationality in terms of engines, rather than arguments.</para>

  
</section>
