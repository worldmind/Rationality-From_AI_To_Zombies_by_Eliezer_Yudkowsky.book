<section xml:id="item_HktFCy6dgsqJ9WPpX"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>Belief in Intelligence</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/HktFCy6dgsqJ9WPpX/belief-in-intelligence"></link>
    </bibliosource>
    <pubdate doc_status="draft">2008-10-25</pubdate>
  </info>
  <indexterm><primary>General Intelligence</primary></indexterm>
  <para>Since I am so uncertain of Kasparov&apos;s moves, what is the empirical content of my belief that &quot;Kasparov is a highly intelligent chess player&quot;?  <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_a7n8GdKiAZRX86T5A">What real-world experience does my belief tell me to anticipate?</olink>  Is it a cleverly masked form of total ignorance? </para>
<para>To sharpen the dilemma, suppose Kasparov plays against some mere chess grandmaster Mr. G, who&apos;s not in the running for world champion.  My own ability is far too low to distinguish between these levels of chess skill.  When I try to guess Kasparov&apos;s move, or Mr. G&apos;s next move, all I can do is try to guess &quot;the best chess move&quot; using my own meager knowledge of chess.  Then I would produce exactly the same prediction for Kasparov&apos;s move or Mr. G&apos;s move in any particular chess position.  So what is the empirical content of my belief that &quot;Kasparov is a <emphasis>better</emphasis> chess player than Mr. G&quot;?</para>
<para>The empirical content of my belief is the testable, falsifiable prediction that the <emphasis>final</emphasis> chess position will occupy the class of chess positions that are wins for Kasparov, rather than drawn games or wins for Mr. G.  (Counting resignation as a legal move that leads to a chess position classified as a loss.)  The degree to which I think Kasparov is a &quot;better player&quot; is reflected in the amount of probability mass I concentrate into the &quot;Kasparov wins&quot; class of outcomes, versus the &quot;drawn game&quot; and &quot;Mr. G wins&quot; class of outcomes.  These classes are extremely vague in the sense that they refer to vast spaces of possible chess positions - but &quot;Kasparov wins&quot; <emphasis>is</emphasis> more specific than maximum entropy, because it can be <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_5JDkW4MYXit2CquLs">definitely falsified</olink> by a vast set of chess positions. </para>
<para>The <emphasis>outcome</emphasis> of Kasparov&apos;s game is predictable because I know, and understand, Kasparov&apos;s goals.  Within the confines of the chess board, I know Kasparov&apos;s motivations - I know his success criterion, his utility function, his target as an optimization process.  I know where Kasparov is <emphasis>ultimately</emphasis> trying to steer the future and I anticipate he is powerful enough to get there, although I don&apos;t anticipate much about <emphasis>how</emphasis> Kasparov is going to do it. </para>
<para>Imagine that I&apos;m visiting a distant city, and a local friend volunteers to drive me to the airport.  I don&apos;t know the neighborhood. Each time my friend approaches a street intersection, I don&apos;t know whether my friend will turn left, turn right, or continue straight ahead.  I can&apos;t predict my friend&apos;s move even as we approach each individual intersection - let alone, predict the whole sequence of moves in advance. </para>
<para>Yet I can predict the <emphasis>result</emphasis> of my friend&apos;s unpredictable actions: we will arrive at the airport.  Even if my friend&apos;s house were located elsewhere in the city, so that my friend made a completely different sequence of turns, I would just as confidently predict our arrival at the airport.  I can predict this long in advance, before I even get into the car.  My flight departs soon, and there&apos;s no time to waste; I wouldn&apos;t get into the car in the first place, if I couldn&apos;t confidently predict that the car would travel to the airport along an unpredictable pathway. </para>
<para>Isn&apos;t this a remarkable situation to be in, from a scientific perspective?  I can predict the <emphasis>outcome</emphasis> of a process, without being able to predict any of the <emphasis>intermediate steps</emphasis> of the process.</para>
<para>How is this even possible?  Ordinarily one predicts by imagining the present and then running the visualization forward in time.  If you want a <emphasis>precise</emphasis> model of the Solar System, one that takes into account planetary perturbations, you must start with a model of all major objects and run that model forward in time, step by step.</para>
<para>Sometimes simpler problems have a closed-form solution, where calculating the future at time T takes the same amount of work regardless of T.  A coin rests on a table, and after each minute, the coin turns over.  The coin starts out showing heads.  What face will it show a hundred minutes later?  Obviously you did not answer this question by visualizing a hundred intervening steps.  You used a closed-form solution that worked to predict the outcome, and would <emphasis>also</emphasis> work to predict any of the intervening steps.</para>
<para>But when my friend drives me to the airport, I can predict the outcome successfully using a strange model that won&apos;t work to predict <emphasis>any</emphasis> of the intermediate steps.  My model doesn&apos;t even require me to input the initial conditions - I don&apos;t need to know where we start out in the city!</para>
<para>I do need to know something about my friend.  I must know that my friend wants me to make my flight.  I must credit that my friend is a good enough planner to successfully drive me to the airport (if he wants to).  These are properties of my <emphasis>friend&apos;s</emphasis> initial state - properties which let me predict the final destination, though not any intermediate turns.</para>
<para>I must also credit that my friend knows enough about the city to drive successfully.  This may be regarded as a relation between my friend and the city; hence, <olink targetdoc="item_ah2GqzZSeBpW9QHgb" targetptr="item_QkX2bAkwG2EpGvNug">a property of both</olink>.  But an extremely <emphasis>abstract</emphasis> property, which does not require any <emphasis>specific</emphasis> knowledge about either the city, or about my friend&apos;s knowledge about the city.</para>
<para>This is one way of viewing the subject matter to which I&apos;ve devoted my life - these <emphasis>remarkable situations</emphasis> which place us in such an odd epistemic positions.  And my work, in a sense, can be viewed as unraveling the exact form of that strange abstract knowledge we can possess; whereby, not knowing the actions, we can justifiably know the consequence.</para>
<para>&quot;Intelligence&quot; is too narrow a term to describe these remarkable situations in full generality.  I would say rather &quot;optimization process&quot;.  A similar situation accompanies the study of biological natural selection, for example; we can&apos;t predict the exact form of the next organism observed.</para>
<para>But my own specialty is the kind of optimization process called &quot;intelligence&quot;; and even narrower, a particular kind of intelligence called &quot;Friendly Artificial Intelligence&quot; - of which, I hope, I will be able to obtain especially precise abstract knowledge.</para>

  
</section>
