<section xml:id="item_i6fKszWY6gLZSX2Ey"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>Fake Optimization Criteria</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/i6fKszWY6gLZSX2Ey/fake-optimization-criteria"></link>
    </bibliosource>
    <pubdate doc_status="draft">2007-11-10</pubdate>
  </info>
  <indexterm><primary>Motivated Reasoning</primary></indexterm>
<indexterm><primary>Optimization</primary></indexterm>
  <para><link xl:href="https://www.lesswrong.com/posts/fkM9XsNvXdYH6PPAx/hindsight-bias">I&apos;ve</link><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_WnheMGAka4fL99eae">previously</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_5JDkW4MYXit2CquLs">dwelt</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_GJ4ZQm7crTzTM6xDW">in</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_mnS2WYLCGJP2kQkRn">considerable</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_jiBFC7DcCrZjGmZnJ">length</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_a7n8GdKiAZRX86T5A">upon</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_CqyJzDZWvGhhFJ7dY">forms</olink><olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_kJiPnaQPiy4p9Eqki">of</olink><olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_SFZoEBpLo9frSJGkc">rationalization</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_RmCjazjupRGcHSm5N">whereby</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_nYkMLFpx77Rz3uo9c">our</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_fAuWLS7RKWD2npBFR">beliefs</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_fysgqk4CjAwhBgNYT">appear </olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_NMoLJuDJEms7Ku9XS">to</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_RgkqLqkg8vLhsYpfh">match</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_FWMfQKG3RpZx6irjm">the</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_6s3xABaXKPdFwA3FS">evidence</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_6i3zToomS86oj9bS6">much</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_8QzZKw9WHRxjR4948">more</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_kpRSCH7ALLcb6ucWM">strongly</olink><olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_rmAbiEKQDpDnZzcRf">than</olink><olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_AdYdLP2sRqPMoe8fb">they</olink><olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_34XxbRFe54FycoCDw">actually</olink><olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_kJiPnaQPiy4p9Eqki">do</olink>. And I&apos;m not overemphasizing the point, either. If we could beat this fundamental metabias and see what every hypothesis <emphasis>really</emphasis> predicted, we would be able to recover from almost any other error of fact.</para>
<para>The mirror challenge for decision theory is seeing which option a choice criterion <emphasis>really</emphasis> endorses. If your <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_bfbiyTogEKWEGP96S">stated moral principles</olink> call for you to provide laptops to everyone, does that <emphasis>really</emphasis> endorse buying a $1 million gem-studded laptop for yourself, or spending the same money on shipping 5000 OLPCs?</para>
<para>We seem to have evolved a knack for arguing that practically any goal implies practically any action. A phlogiston theorist explaining why magnesium gains weight when burned has nothing on an Inquisitor explaining why God&apos;s infinite love for all His children requires burning some of them at the stake.</para>
<para>There&apos;s no mystery about this. <olink targetdoc="item_coGq3LC5Yn4vZiu6k" targetptr="item_9weLK2AJ9JEt2Tt8f">Politics</olink> was a feature of the ancestral environment. We are descended from those who argued most persuasively that the good of the tribe meant executing their hated rival Uglak. (We sure ain&apos;t descended from Uglak.) </para>
<para>And yet... is it possible to <emphasis>prove</emphasis> that if Robert Mugabe cared <emphasis>only</emphasis> for the good of Zimbabwe, he would resign from its presidency? You can <emphasis>argue</emphasis> that the policy follows from the goal, but haven&apos;t we just seen that humans can match up any goal to any policy? How do you know that you&apos;re right and Mugabe is wrong? (There are a number of reasons this is a good guess, but bear with me here.)</para>
<para>Human motives are manifold and obscure, our decision processes as vastly complicated as our brains. And the world itself is vastly complicated, on every choice of real-world policy. Can we even <emphasis>prove</emphasis> that human beings are rationalizing—that we&apos;re systematically distorting the link from principles to policy—when we lack a single firm place on which to stand? When there&apos;s no way to find out <emphasis>exactly</emphasis> what even a single optimization criterion implies? (Actually, you can just observe that people <emphasis>disagree</emphasis> about office politics in ways that strangely correlate to their own interests, while simultaneously denying that any such interests are at work. But again, bear with me here.)</para>
<para>Where is the standardized, open-source, generally intelligent, consequentialist optimization process into which we can feed a complete morality as an XML file, to find out what that morality <emphasis>really</emphasis> recommends when applied to our world? Is there even a single real-world case where we can know <emphasis>exactly</emphasis> what a choice criterion recommends? Where is the <emphasis>pure</emphasis> moral reasoner—of known utility function, purged of all other stray desires that might distort its optimization—whose trustworthy output we can contrast to human rationalizations of the same utility function?</para>
<para>Why, it&apos;s our old friend the <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_pLRogvJLPPg6Mrvg4">alien god</olink>, of course! Natural selection is guaranteed free of all mercy, all love, all compassion, all aesthetic sensibilities, all political factionalism, all ideological allegiances, all academic ambitions, all libertarianism, all socialism, <olink targetdoc="item_PYQ2izdfDPTe5uJTG" targetptr="item_6hfGNLf4Hg5DXqJCF">all Blue and all Green</olink>. Natural selection doesn&apos;t <emphasis>maximize</emphasis> its criterion of inclusive genetic fitness—it&apos;s <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_jAToJHtg39AMTAuJo">not that smart</olink>. But when you look at the output of natural selection, you are guaranteed to be looking at an output that was optimized <emphasis>only</emphasis> for inclusive genetic fitness, and not the interests of the US agricultural industry.</para>
<para>In the case histories of evolutionary science—in, for example, <olink targetdoc="item_wAXodw6LPScjrdnkR" targetptr="item_QsMJQSFj7WfoTMNgW">The Tragedy of Group Selectionism</olink>—we can directly compare human rationalizations to the result of<emphasis> pure</emphasis> optimization for a known criterion. What did Wynne-Edwards think would be the result of group selection for small subpopulation sizes? Voluntary individual restraint in breeding, and enough food for everyone. What was the actual laboratory result? Cannibalism.</para>
<para>Now you might ask: Are these case histories of evolutionary science really relevant to human morality, which doesn&apos;t give two figs for inclusive genetic fitness when it gets in the way of love, compassion, aesthetics, healing, freedom, fairness, et cetera? Human societies didn&apos;t even have a concept of &quot;inclusive genetic fitness&quot; until the 20th century.</para>
<para>But I ask in return: If we can&apos;t see clearly the result of a single monotone optimization criterion—if we can&apos;t even train ourselves to hear a single pure note—then how will we listen to an orchestra? How will we see that &quot;Always be selfish&quot; or &quot;Always obey the government&quot; are poor guiding principles for human beings to adopt—if we think that even <emphasis>optimizing genes for inclusive fitness</emphasis> will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?</para>
<para>To train ourselves to see clearly, we need simple practice cases.</para>

  
</section>
