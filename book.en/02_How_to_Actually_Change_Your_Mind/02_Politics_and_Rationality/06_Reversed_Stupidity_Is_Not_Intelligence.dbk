<section xml:id="item_qNZM3EGoE5ZeMdCRt"
      xmlns="http://docbook.org/ns/docbook"
      version="5.0"
      xml:lang="en"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:xl="http://www.w3.org/1999/xlink">
  <info>
    <title>Reversed Stupidity Is Not Intelligence</title>
    <bibliosource class="uri">
      <link xlink:href="https://www.lesswrong.com/posts/qNZM3EGoE5ZeMdCRt/reversed-stupidity-is-not-intelligence"></link>
    </bibliosource>
    <pubdate doc_status="draft">2020-07-23</pubdate>
  </info>
  <indexterm><primary>Distinctions</primary></indexterm>
<indexterm><primary>Rationality</primary></indexterm>
<indexterm><primary>Steelmanning</primary></indexterm>
<indexterm><primary>Principles</primary></indexterm>
<indexterm><primary>Epistemology</primary></indexterm>
<indexterm><primary>Reversed Stupidity Is Not Intelligence</primary></indexterm>
  <blockquote>
  <para> “. . . then our people on that time-line went to work with corrective action. Here.”</para>
  <para>He wiped the screen and then began punching combinations. Page after page appeared, bearing accounts of people who had claimed to have seen the mysterious disks, and each report was more fantastic than the last.</para>
  <para>“The standard smother-out technique,” Verkan Vall grinned. “I only heard a little talk about the ‘flying saucers,’ and all of that was in joke. In that order of culture, you can always discredit one true story by setting up ten others, palpably false, parallel to it.”</para>
  <para>—H. Beam Piper, <emphasis>Police Operation</emphasis></para>
</blockquote>
<para>Piper had a point. Pers’nally, I don’t believe there are any poorly hidden aliens infesting these parts. But my disbelief has nothing to do with the awful embarrassing irrationality of flying saucer cults—at least, I hope not.</para>
<para>You and I believe that flying saucer cults arose in the total absence of any flying saucers. Cults can arise around almost any idea, thanks to human silliness. This silliness operates <emphasis>orthogonally</emphasis> to alien intervention: We would expect to see flying saucer cults whether or not there were flying saucers. Even if there were poorly hidden aliens, it would not be any <emphasis>less</emphasis> likely for flying saucer cults to arise. The conditional probability P(cults|aliens) isn’t less than P(cults|¬aliens), unless you suppose that poorly hidden aliens would deliberately suppress flying saucer cults.<footnote xml:id="fn_1f695af4ffbc51eb7f8d77e468f3b51f">
    <para>Read “P(cults|aliens)” as “the probability of UFO cults given that aliens have visited Earth,” and read “P(cults|¬aliens)” as “the probability of UFO cults given that aliens have not visited Earth.”</para>
  </footnote>
<anchor xml:id="x24-25001f1"/> By the Bayesian definition of evidence, the observation “flying saucer cults exist” is not evidence  the existence of flying saucers. It’s not much evidence one way or the other.</para>
<para>This is an application of the general principle that, as Robert Pirsig puts it, “The world’s greatest fool may say the Sun is shining, but that doesn’t make it dark out.”<footnote xml:id="fn_72ef32c4208821c1e0940d55cf1b925c">
    <para><anchor xml:id="cite.0.Pirsig.1974"/>Robert M. Pirsig, <emphasis>Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values</emphasis>, 1st ed. (New York: Morrow, 1974).</para>
  </footnote>
<anchor xml:id="x24-25002f2"/></para>
<para>If you knew someone who was wrong 99.99% of the time on yes-or-no questions, you could obtain 99.99% accuracy just by reversing their answers. They would need to do all the work of obtaining good evidence entangled with reality, and processing that evidence coherently, just to <emphasis>anticorrelate</emphasis> that reliably. They would have to be superintelligent to be that stupid.</para>
<para>A car with a broken engine cannot drive backward at 200 mph, even if the engine is <emphasis>really really broken.</emphasis></para>
<para>If stupidity does not reliably anticorrelate with truth, how much less should human evil anticorrelate with truth? The converse of the halo effect is the horns effect: All perceived negative qualities correlate. If Stalin is evil, then everything he says should be false. You wouldn’t want to agree with <emphasis>Stalin</emphasis>, would you?</para>
<para>Stalin also believed that 2 + 2 = 4. Yet if you defend any statement made by Stalin, even “2 + 2 = 4,” people will see only that you are “agreeing with Stalin”; you must be on his side.</para>
<para>Corollaries of this principle:</para>
<itemizedlist>
  <listitem>
    <para> To argue against an idea honestly, you should argue against the best arguments of the strongest advocates. Arguing against weaker advocates proves <emphasis>nothing</emphasis>, because even the strongest idea will attract weak advocates. If you want to argue against transhumanism or the intelligence explosion, you have to directly challenge the arguments of Nick Bostrom or Eliezer Yudkowsky post-2003. The least convenient path is the only valid one.<footnote xml:id="fn_b3e5b07ae0fe78226b4f400a95d26423">
        <para>See <anchor xml:id="cite.0.Alexander.2009"/>Scott Alexander, “The Least Convenient Possible World,” <emphasis>Less Wrong</emphasis> (blog), December 2, 2018,<link xl:href="https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world">http://lesswrong.com/lw/2k/the_least_convenient_possible_world/</link>.</para>
      </footnote>
<anchor xml:id="x24-25003f3"/></para>
  </listitem>
  <listitem>
    <para> Exhibiting sad, pathetic lunatics, driven to madness by their apprehension of an Idea, is no evidence against that Idea. Many New Agers have been made crazier by their personal apprehension of quantum mechanics.</para>
  </listitem>
  <listitem>
    <para> Someone once said, “Not all conservatives are stupid, but most stupid people are conservatives.” If you cannot place yourself in a state of mind where this statement, true or false, seems <emphasis>completely irrelevant</emphasis> as a critique of conservatism, you are not ready to think rationally about politics.</para>
  </listitem>
  <listitem>
    <para> Ad hominem argument is not valid.</para>
  </listitem>
  <listitem>
    <para> You need to be able to argue against genocide without saying “Hitler wanted to exterminate the Jews.” If Hitler <emphasis>hadn’t</emphasis> advocated genocide, would it thereby become okay?</para>
  </listitem>
  <listitem>
    <para> In Hansonian terms: Your instinctive willingness to believe something will change along with your willingness to <emphasis>affiliate</emphasis> with people who are known for believing it—quite apart from whether the belief is actually <emphasis>true.</emphasis> Some people may be reluctant to believe that God does not exist, not because there is evidence that God <emphasis>does</emphasis> exist, but rather because they are reluctant to affiliate with Richard Dawkins or those darned “strident” atheists who go around publicly saying “God does not exist.”</para>
  </listitem>
  <listitem>
    <para> If your current computer stops working, you can’t conclude that everything about the current system is wrong and that you need a new system without an AMD processor, an ATI video card, a Maxtor hard drive, or case fans—even though your current system has all these things and it doesn’t work. Maybe you just need a new power cord.</para>
  </listitem>
  <listitem>
    <para> If a hundred inventors fail to build flying machines using metal and wood and canvas, it doesn’t imply that what you really need is a flying machine of bone and flesh. If a thousand projects fail to build Artificial Intelligence using electricity-based computing, this doesn’t mean that electricity is the source of the problem. Until you understand the problem, hopeful reversals are exceedingly unlikely to hit the solution.<footnote xml:id="fn_ab5b4a44e648ca53492b923f155d3685">
        <para>See also “Selling Nonapples.” <link xl:href="https://www.lesswrong.com/posts/2mLZiWxWKZyaRgcn7/selling-nonapples">http://lesswrong.com/lw/vs/selling_nonapples</link>.</para>
      </footnote>
<anchor xml:id="x24-25004f4"/></para>
  </listitem>
</itemizedlist>

  
</section>
