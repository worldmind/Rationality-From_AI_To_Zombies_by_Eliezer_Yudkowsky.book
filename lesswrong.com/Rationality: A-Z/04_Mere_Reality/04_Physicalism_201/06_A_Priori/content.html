<p>Traditional Rationality is phrased as social rules, with violations interpretable as cheating: if you break the rules and no one else is doing so, you're the first to defect - making you a bad, bad person.&nbsp; To Bayesians, the brain is an engine of accuracy: <a href="https://www.lesswrong.com/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/">if you violate the laws of rationality, the engine doesn't run</a>, and this is equally true whether anyone else breaks the rules or not.</p>

<p>Consider the problem of <a href="https://www.lesswrong.com/lw/jp/occams_razor/">Occam's Razor</a>, as confronted by Traditional philosophers.&nbsp; If two hypotheses fit the same observations <a href="https://www.lesswrong.com/lw/jp/occams_razor/">equally well</a>, why believe the simpler one is more likely to be true?</p><a id="more"></a><p>You could argue that Occam's Razor has worked in the past,
and is therefore likely to continue to work in the future.&nbsp; But
this, itself, appeals to a prediction from Occam's Razor.&nbsp; &quot;Occam's Razor works up to
October 8th, 2007 and then stops working thereafter&quot; is more complex,
but it fits the observed evidence equally well.</p>

<p>You could argue that Occam's Razor is a reasonable distribution on
prior probabilities.&nbsp; But what is a &quot;reasonable&quot; distribution?&nbsp; Why not label &quot;reasonable&quot; a very complicated prior distribution,
which makes Occam's Razor work in all observed tests so far, but
generates exceptions in future cases?</p>

<p>Indeed, it seems there is no way to <em>justify</em> Occam's Razor except by <em>appealing</em> to Occam's Razor, making this <em>argument</em> unlikely to <em>convince</em> any <em>judge</em> who does not already <em>accept</em> Occam's Razor.&nbsp; (What's special about the words I italicized?)</p>

<p>If you are a philosopher whose daily work is to write papers,
criticize other people's papers, and respond to others' criticisms of
your own papers, then you may look at Occam's Razor and shrug.&nbsp; Here is an end to justifying, arguing and convincing.&nbsp; You decide to call a <a href="https://www.lesswrong.com/lw/it/semantic_stopsigns/">truce</a> on writing papers; if your fellow philosophers do not demand justification for your un-arguable beliefs, you will not demand justification for theirs.&nbsp; And as the symbol of your treaty, your white flag, you use the phrase &quot;<a href="https://www.lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/">a priori truth</a>&quot;.</p>

<p>But to a Bayesian, in this era of cognitive science and evolutionary biology and Artificial Intelligence, saying &quot;a priori&quot; doesn't explain why the brain-engine
runs.&nbsp; If the brain has an amazing &quot;a priori truth factory&quot; that <em>works</em>
to produce accurate beliefs, it makes you wonder why a thirsty
hunter-gatherer can't use the &quot;a priori truth factory&quot; to locate
drinkable water.&nbsp; It makes you wonder why eyes evolved in the first place, if there are ways to produce accurate beliefs without <a href="https://www.lesswrong.com/lw/jl/what_is_evidence/">looking at things</a>.</p>

<p>James R. Newman said:&nbsp; &quot;The fact that one apple added to one apple invariably gives two apples helps in the teaching of arithmetic, but has no bearing on the truth of the proposition that 1 + 1 = 2.&quot;&nbsp; The Internet Encyclopedia of Philosophy <a href="http://www.iep.utm.edu/a/apriori.htm">defines</a> &quot;a priori&quot; propositions as those knowable independently of experience.&nbsp; Wikipedia <a href="http://en.wikipedia.org/wiki/A_priori_and_a_posteriori_%28philosophy%29">quotes</a> Hume:&nbsp; Relations of ideas are &quot;discoverable by the mere operation of thought, without dependence on what is anywhere existent in the universe.&quot;&nbsp; You can see that 1 + 1 = 2 <em>just by thinking about it,</em> without looking at apples.</p>

<p>But in this era of neurology, one ought to be aware that <em>thoughts</em> are existent in the universe; they are identical to the operation of brains.&nbsp; Material brains, real in the universe, composed of quarks in a single unified mathematical physics whose laws draw no border between the inside and outside of your skull.</p>

<p>When you add 1 + 1 and get 2 by thinking, these thoughts are themselves embodied in flashes of neural patterns.&nbsp; In principle, we could <em>observe</em>, experientially, the exact same material events as they occurred within someone else's brain.&nbsp; It would require some advances in computational neurobiology and brain-computer interfacing, but in principle, it could be done.&nbsp; You could see someone else's engine operating materially, through material chains of cause and effect, to compute by &quot;pure thought&quot; that 1 + 1 = 2.&nbsp; How is observing this pattern in <em>someone else's</em> brain any different, as a way of knowing, from observing your own brain doing the same thing?&nbsp; When &quot;pure thought&quot; tells you that 1 + 1 = 2, &quot;independently of any experience or observation&quot;, you are, in effect, observing your own brain as evidence.</p>

<p>If this seems counterintuitive, try to see minds/brains as engines - an engine that collides the neural pattern for 1 and the neural pattern for 1 and gets the neural pattern for 2.&nbsp; If this engine works at all, then it should <em>have the same output</em> if it observes (with eyes and retina) a similar brain-engine carrying out a similar collision, and copies into itself the resulting pattern.&nbsp; In other words, for every form of a priori knowledge obtained by &quot;pure thought&quot;, you are learning exactly the same thing you would learn if you saw an outside brain-engine carrying out the same pure flashes of neural activation.&nbsp; The engines are equivalent, the <a href="https://www.lesswrong.com/lw/js/the_bottom_line/">bottom-line outputs</a> are equivalent, the <a href="https://www.lesswrong.com/lw/jl/what_is_evidence/">belief-entanglements</a> are the same.</p>

<p>There is nothing you can know &quot;a priori&quot;, which you could not know with equal validity by observing the chemical release of neurotransmitters within some outside brain.&nbsp; What do you think you <em>are,</em> dear reader?</p>

<p>This is <em>why</em> you can predict the result of adding 1 apple and 1 apple by imagining it first in your mind, or punch &quot;3 x 4&quot; into a calculator to predict the result of imagining 4 rows with 3 apples per row.&nbsp; You and the apple exist within a <a href="https://www.lesswrong.com/lw/hr/universal_law/">boundary-less unified physical process</a>, and one part may echo another.</p>

<p>Are the sort of neural flashes that philosophers label &quot;a priori beliefs&quot;, <em>arbitrary</em>?&nbsp; Many AI algorithms function better with &quot;regularization&quot; that biases
the solution space toward simpler solutions.&nbsp; But the regularized
algorithms are themselves more complex; they contain an extra line of
code (or 1000 extra lines) compared to unregularized algorithms.&nbsp; The human
brain is biased toward simplicity,
and we think more efficiently thereby.&nbsp; If you <a href="https://www.lesswrong.com/lw/j2/explainworshipignore/">press the Ignore button</a>
at this point, you're left with a complex brain that exists
for no reason and works for no reason.&nbsp; So don't try to tell me that &quot;a priori&quot; beliefs are
arbitrary, because they sure aren't generated by rolling random numbers.&nbsp; (What does the adjective &quot;arbitrary&quot; <em>mean,</em> anyway?)</p>

<p>You can't <a href="https://www.lesswrong.com/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/">excuse</a> calling a proposition &quot;a priori&quot; by pointing out that
<em>other</em> philosophers are having trouble justifying <em>their</em> propositions.&nbsp; If a
philosopher fails to explain something, this fact cannot supply electricity to a refrigerator, nor act as a magical factory for
accurate beliefs.&nbsp; There's no truce, no white flag, until you understand why the engine
works.</p>

<p>If you clear your mind of <em>justification,</em> of <em>argument,</em> then it seems obvious why Occam's Razor works in practice: we live in a simple world, a low-entropy universe in which there are short explanations to be found.&nbsp; &quot;But,&quot; you cry, &quot;why is the universe itself orderly?&quot;&nbsp; This I do not know, but it is what I see as the next mystery to be <a href="https://www.lesswrong.com/lw/j2/explainworshipignore/">explained</a>.&nbsp; This is not the same question as &quot;How do I argue Occam's Razor to a hypothetical debater who has not already accepted it?&quot;</p>

<p>Perhaps you cannot argue <em>anything</em> to a hypothetical debater who has not accepted Occam's Razor, just as you cannot argue anything to a rock.&nbsp; A mind needs a certain amount of dynamic structure to be an argument-acceptor.&nbsp; If a mind doesn't implement Modus Ponens, it can accept &quot;A&quot; and &quot;A-&gt;B&quot; all day long without ever producing &quot;B&quot;.&nbsp; How do you justify Modus Ponens to a mind that hasn't accepted it?&nbsp; How do you argue a rock into becoming a mind?</p>

<p>Brains evolved from non-brainy matter by natural selection; they were not justified into existence by arguing with an ideal philosophy student of perfect emptiness.&nbsp; This does not make our judgments meaningless.&nbsp; A brain-engine can work correctly, producing accurate beliefs, even if it was merely <em>built</em> - by human hands or cumulative stochastic selection pressures - rather than argued into existence.&nbsp; But to be satisfied by this answer, one must see rationality in terms of engines, rather than arguments.</p>