<p><a href="https://www.lesswrong.com/lw/il/hindsight_bias/">I&#x27;ve</a> <a href="https://www.lesswrong.com/lw/im/hindsight_devalues_science/">previously</a> <a href="https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/">dwelt</a> <a href="https://www.lesswrong.com/lw/ia/focus_your_uncertainty/">in</a> <a href="https://www.lesswrong.com/lw/ih/absence_of_evidence_is_evidence_of_absence/">considerable</a> <a href="https://www.lesswrong.com/lw/ii/conservation_of_expected_evidence/">length</a> <a href="https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/">upon</a> <a href="https://www.lesswrong.com/lw/i4/belief_in_belief/">forms</a> <a href="https://www.lesswrong.com/lw/jt/what_evidence_filtered_evidence/">of</a> <a href="https://www.lesswrong.com/lw/ju/rationalization/">rationalization</a> <a href="https://www.lesswrong.com/lw/i6/professing_and_cheering/">whereby</a> <a href="https://www.lesswrong.com/lw/i7/belief_as_attire/">our</a> <a href="https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/">beliefs</a> <a href="https://www.lesswrong.com/lw/ip/fake_explanations/">appear </a><a href="https://www.lesswrong.com/lw/iq/guessing_the_teachers_password/">to</a> <a href="https://www.lesswrong.com/lw/is/fake_causality/">match</a> <a href="https://www.lesswrong.com/lw/it/semantic_stopsigns/">the</a> <a href="https://www.lesswrong.com/lw/jl/what_is_evidence/">evidence</a> <a href="https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/">much</a> <a href="https://www.lesswrong.com/lw/iv/the_futility_of_emergence/">more</a> <a href="https://www.lesswrong.com/lw/ix/say_not_complexity/">strongly</a> <a href="https://www.lesswrong.com/lw/iw/positive_bias_look_into_the_dark/">than</a> <a href="https://www.lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/">they</a> <a href="https://www.lesswrong.com/lw/js/the_bottom_line/">actually</a> <a href="https://www.lesswrong.com/lw/jt/what_evidence_filtered_evidence/">do</a>.  And I&#x27;m not overemphasizing the point, either.  If we could beat this fundamental metabias and see what every hypothesis <em>really</em> predicted, we would be able to recover from almost any other error of fact.</p><p>The mirror challenge for decision theory is seeing which option a choice criterion <em>really</em> endorses.  If your <a href="https://www.lesswrong.com/lw/kq/fake_justification/">stated moral principles</a> call for you to provide laptops to everyone, does that <em>really</em> endorse buying a $1 million gem-studded laptop for yourself, or spending the same money on shipping 5000 OLPCs?</p><p>We seem to have evolved a knack for arguing that practically any goal implies practically any action.  A phlogiston theorist explaining why magnesium gains weight when burned has nothing on an Inquisitor explaining why God&#x27;s infinite love for all His children requires burning some of them at the stake.</p><p>There&#x27;s no mystery about this.  <a href="https://www.lesswrong.com/lw/gw/politics_is_the_mindkiller/">Politics</a> was a feature of the ancestral environment.  We are descended from those who argued most persuasively that the good of the tribe meant executing their hated rival Uglak.  (We sure ain&#x27;t descended from Uglak.) </p><p></p><p>And yet... is it possible to <em>prove</em> that if Robert Mugabe cared <em>only</em> for the good of Zimbabwe, he would resign from its presidency?  You can <em>argue</em> that the policy follows from the goal, but haven&#x27;t we just seen that humans can match up any goal to any policy?  How do you know that you&#x27;re right and Mugabe is wrong?  (There are a number of reasons this is a good guess, but bear with me here.)</p><p>Human motives are manifold and obscure, our decision processes as vastly complicated as our brains.  And the world itself is vastly complicated, on every choice of real-world policy.  Can we even <em>prove</em> that human beings are rationalizing—that we&#x27;re systematically distorting the link from principles to policy—when we lack a single firm place on which to stand?  When there&#x27;s no way to find out <em>exactly</em> what even a single optimization criterion implies?  (Actually, you can just observe that people <em>disagree</em> about office politics in ways that strangely correlate to their own interests, while simultaneously denying that any such interests are at work.  But again, bear with me here.)</p><p>Where is the standardized, open-source, generally intelligent, consequentialist optimization process into which we can feed a complete morality as an XML file, to find out what that morality <em>really</em> recommends when applied to our world?  Is there even a single real-world case where we can know <em>exactly</em> what a choice criterion recommends?  Where is the <em>pure</em> moral reasoner—of known utility function, purged of all other stray desires that might distort its optimization—whose trustworthy output we can contrast to human rationalizations of the same utility function?</p><p>Why, it&#x27;s our old friend the <a href="https://www.lesswrong.com/lw/kr/an_alien_god/">alien god</a>, of course!  Natural selection is guaranteed free of all mercy, all love, all compassion, all aesthetic sensibilities, all political factionalism, all ideological allegiances, all academic ambitions, all libertarianism, all socialism, <a href="https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/">all Blue and all Green</a>.  Natural selection doesn&#x27;t <em>maximize</em> its criterion of inclusive genetic fitness—it&#x27;s <a href="https://www.lesswrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/">not that smart</a>.  But when you look at the output of natural selection, you are guaranteed to be looking at an output that was optimized <em>only</em> for inclusive genetic fitness, and not the interests of the US agricultural industry.</p><p>In the case histories of evolutionary science—in, for example, <a href="https://www.lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/">The Tragedy of Group Selectionism</a>—we can directly compare human rationalizations to the result of<em> pure</em> optimization for a known criterion.  What did Wynne-Edwards think would be the result of group selection for small subpopulation sizes?  Voluntary individual restraint in breeding, and enough food for everyone.  What was the actual laboratory result?  Cannibalism.</p><p>Now you might ask:  Are these case histories of evolutionary science really relevant to human morality, which doesn&#x27;t give two figs for inclusive genetic fitness when it gets in the way of love, compassion, aesthetics, healing, freedom, fairness, et cetera?  Human societies didn&#x27;t even have a concept of &quot;inclusive genetic fitness&quot; until the 20th century.</p><p>But I ask in return:  If we can&#x27;t see clearly the result of a single monotone optimization criterion—if we can&#x27;t even train ourselves to hear a single pure note—then how will we listen to an orchestra?  How will we see that &quot;Always be selfish&quot; or &quot;Always obey the government&quot; are poor guiding principles for human beings to adopt—if we think that even <em>optimizing genes for inclusive fitness</em> will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?</p><p>To train ourselves to see clearly, we need simple practice cases.</p>