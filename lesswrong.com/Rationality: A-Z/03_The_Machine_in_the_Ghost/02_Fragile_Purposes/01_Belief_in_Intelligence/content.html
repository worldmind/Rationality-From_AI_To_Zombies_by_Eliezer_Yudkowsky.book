<p>Since I am so uncertain of Kasparov's moves, what is the empirical content of my belief that &quot;Kasparov is a highly intelligent chess player&quot;?&nbsp; <a href="/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/">What real-world experience does my belief tell me to anticipate?</a>&nbsp; Is it a cleverly masked form of total ignorance? </p>

<p>To sharpen the dilemma, suppose Kasparov plays against some mere chess grandmaster Mr. G, who's not in the running for world champion.&nbsp; My own ability is far too low to distinguish between these levels of chess skill.&nbsp; When I try to guess Kasparov's move, or Mr. G's next move, all I can do is try to guess &quot;the best chess move&quot; using my own meager knowledge of chess.&nbsp; Then I would produce exactly the same prediction for Kasparov's move or Mr. G's move in any particular chess position.&nbsp; So what is the empirical content of my belief that &quot;Kasparov is a <em>better</em> chess player than Mr. G&quot;?</p><a id="more"></a><p>The empirical content of my belief is the testable, falsifiable prediction that the <em>final</em> chess position will occupy the class of chess positions that are wins for Kasparov, rather than drawn games or wins for Mr. G.&nbsp; (Counting resignation as a legal move that leads to a chess position classified as a loss.)&nbsp; The degree to which I think Kasparov is a &quot;better player&quot; is reflected in the amount of probability mass I concentrate into the &quot;Kasparov wins&quot; class of outcomes, versus the &quot;drawn game&quot; and &quot;Mr. G wins&quot; class of outcomes.&nbsp; These classes are extremely vague in the sense that they refer to vast spaces of possible chess positions - but &quot;Kasparov wins&quot; <em>is</em> more specific than maximum entropy, because it can be <a href="/lw/if/your_strength_as_a_rationalist/">definitely falsified</a> by a vast set of chess positions. </p>

<p>The <em>outcome</em> of Kasparov's game is predictable because I know, and understand, Kasparov's goals.&nbsp; Within the confines of the chess board, I know Kasparov's motivations - I know his success criterion, his utility function, his target as an optimization process.&nbsp; I know where Kasparov is <em>ultimately</em> trying to steer the future and I anticipate he is powerful enough to get there, although I don't anticipate much about <em>how</em> Kasparov is going to do it. </p>

<p>Imagine that I'm visiting a distant city, and a local friend volunteers to drive me to the airport.&nbsp; I don't know the neighborhood. Each time my friend approaches a street intersection, I don't know whether my friend will turn left, turn right, or continue straight ahead.&nbsp; I can't predict my friend's move even as we approach each individual intersection - let alone, predict the whole sequence of moves in advance. </p>

<p>Yet I can predict the <em>result</em> of my friend's unpredictable actions: we will arrive at the airport.&nbsp; Even if my friend's house were located elsewhere in the city, so that my friend made a completely different sequence of turns, I would just as confidently predict our arrival at the airport.&nbsp; I can predict this long in advance, before I even get into the car.&nbsp; My flight departs soon, and there's no time to waste; I wouldn't get into the car in the first place, if I couldn't confidently predict that the car would travel to the airport along an unpredictable pathway. </p>

<p>Isn't this a remarkable situation to be in, from a scientific perspective?&nbsp; I can predict the <em>outcome</em> of a process, without being able to predict any of the <em>intermediate steps</em> of the process.</p>

<p>How is this even possible?&nbsp; Ordinarily one predicts by imagining the present and then running the visualization forward in time.&nbsp; If you want a <em>precise</em> model of the Solar System, one that takes into account planetary perturbations, you must start with a model of all major objects and run that model forward in time, step by step.</p>

<p>Sometimes simpler problems have a closed-form solution, where calculating the future at time T takes the same amount of work regardless of T.&nbsp; A coin rests on a table, and after each minute, the coin turns over.&nbsp; The coin starts out showing heads.&nbsp; What face will it show a hundred minutes later?&nbsp; Obviously you did not answer this question by visualizing a hundred intervening steps.&nbsp; You used a closed-form solution that worked to predict the outcome, and would <em>also</em> work to predict any of the intervening steps.</p>

<p>But when my friend drives me to the airport, I can predict the outcome successfully using a strange model that won't work to predict <em>any</em> of the intermediate steps.&nbsp; My model doesn't even require me to input the initial conditions - I don't need to know where we start out in the city!</p>

<p>I do need to know something about my friend.&nbsp; I must know that my friend wants me to make my flight.&nbsp; I must credit that my friend is a good enough planner to successfully drive me to the airport (if he wants to).&nbsp; These are properties of my <em>friend's</em> initial state - properties which let me predict the final destination, though not any intermediate turns.</p>

<p>I must also credit that my friend knows enough about the city to drive successfully.&nbsp; This may be regarded as a relation between my friend and the city; hence, <a href="/lw/o5/the_second_law_of_thermodynamics_and_engines_of/">a property of both</a>.&nbsp; But an extremely <em>abstract</em> property, which does not require any <em>specific</em> knowledge about either the city, or about my friend's knowledge about the city.</p>

<p>This is one way of viewing the subject matter to which I've devoted my life - these <em>remarkable situations</em> which place us in such an odd epistemic positions.&nbsp; And my work, in a sense, can be viewed as unraveling the exact form of that strange abstract knowledge we can possess; whereby, not knowing the actions, we can justifiably know the consequence.</p>

<p>&quot;Intelligence&quot; is too narrow a term to describe these remarkable situations in full generality.&nbsp; I would say rather &quot;optimization process&quot;.&nbsp; A similar situation accompanies the study of biological natural selection, for example; we can't predict the exact form of the next organism observed.</p>

<p>But my own specialty is the kind of optimization process called &quot;intelligence&quot;; and even narrower, a particular kind of intelligence called &quot;Friendly Artificial Intelligence&quot; - of which, I hope, I will be able to obtain especially precise abstract knowledge.</p>