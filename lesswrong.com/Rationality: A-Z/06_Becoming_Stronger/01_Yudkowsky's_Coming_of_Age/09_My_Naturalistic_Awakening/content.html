<p>In yesterday's episode, Eliezer<sub>2001</sub> is fighting a rearguard action against the truth.&nbsp; Only gradually shifting his beliefs, admitting an increasing probability in a different scenario, but never saying outright, "I was wrong before."&nbsp; He repairs his strategies as they are challenged, finding new justifications for just the same plan he pursued before.</p>
<p>(Of which it is therefore said:&nbsp; "Beware lest you fight a rearguard retreat against the evidence, grudgingly conceding each foot of ground only when forced, feeling cheated.&nbsp; Surrender to the truth as quickly as you can.&nbsp; Do this the instant you realize what you are resisting; the instant you can see from which quarter the winds of evidence are blowing against you.")</p>
<p>Memory fades, and I can hardly bear to look back upon those times&mdash;no, seriously, I can't <em>stand</em> reading my old writing.&nbsp; I've already been corrected once in my recollections, by those who were present.&nbsp; And so, though I remember the important events, I'm not really sure what <em>order</em> they happened in, let alone what year.</p>
<p>But if I had to pick a moment when my folly broke, I would pick the moment when I first comprehended, in full generality, <a href="https://www.lesswrong.com/lw/tx/optimization/">the notion of an optimization process</a>.&nbsp; That was the point at which I first looked back and said, "I've been a fool."</p>
<p><a id="more"></a></p>
<p>Previously, in 2002, I'd been writing a bit about the evolutionary psychology of human general intelligence&mdash;though at the time, I thought I was writing about AI; at this point I thought I was against anthropomorphic intelligence, but I was still looking to the human brain for inspiration.&nbsp; (The paper in question is "Levels of Organization in General Intelligence", a requested chapter for the volume "Artificial General Intelligence", which finally came out in print in 2007.)</p>
<p>So I'd been thinking (and writing) about how natural selection managed to cough up human intelligence; I saw a <em>dichotomy</em> between them, the blindness of natural selection and the lookahead of intelligent foresight, reasoning by simulation versus playing everything out in reality, abstract versus concrete thinking.&nbsp; And yet it was natural selection that created human intelligence, so that our brains, though not our thoughts, are entirely made according to the signature of natural selection.</p>
<p>To this day, this still seems to me like a reasonably shattering insight, and so it drives me up the wall when people lump together natural selection and intelligence-driven processes as "evolutionary". They really are almost absolutely different in a number of important ways&mdash;though there are concepts in common that can be used to describe them, like consequentialism and cross-domain generality.</p>
<p>But that Eliezer<sub>2002</sub> is thinking in terms of a <em>dichotomy</em> between evolution and intelligence tells you something about the limits of his vision&mdash;like someone who thinks of politics as a <a href="https://www.lesswrong.com/lw/mg/the_twoparty_swindle/">dichotomy</a> between conservative and liberal stances, or someone who thinks of fruit as a dichotomy between apples and strawberries.</p>
<p>After the "Levels of Organization" draft was published online, Emil Gilliam pointed out that my view of AI seemed pretty similar to my view of intelligence.&nbsp; Now, of course Eliezer<sub>2002</sub> doesn't espouse building an AI in the image of a human mind; Eliezer<sub>2002</sub> knows very well that a human mind is just a hack coughed up by natural selection.&nbsp; But Eliezer<sub>2002</sub> has described these levels of organization in human thinking, and he hasn't proposed using different levels of organization in the AI.&nbsp; Emil Gilliam asks whether I think I might be hewing too close to the human line.&nbsp; I dub the alternative the "Completely Alien Mind Design" and reply that a CAMD is probably too difficult for human engineers to create, even if it's possible in theory, because we wouldn't be able to understand something so alien while we were putting it together.</p>
<p>I don't know if Eliezer<sub>2002</sub> invented this reply on his own, or if he <a href="https://www.lesswrong.com/lw/k5/cached_thoughts/">read it somewhere else</a>. Needless to say, I've heard this excuse plenty of times since then.&nbsp; In reality, what you genuinely understand, you can usually reconfigure in almost any sort of shape, leaving some <a href="https://www.lesswrong.com/lw/o7/searching_for_bayesstructure/">structural essence</a> inside; but when you don't understand flight, you suppose that a flying machine needs feathers, because <a href="https://www.lesswrong.com/lw/tf/dreams_of_ai_design/">you can't imagine departing</a> from the <a href="https://www.lesswrong.com/lw/rj/surface_analogies_and_deep_causes/">analogy</a> of a bird.</p>
<p>So Eliezer<sub>2002</sub> is still, in a sense, attached to humanish mind designs&mdash;he imagines improving on them, but the human <em>architecture</em> is still in some sense his <a href="https://www.lesswrong.com/lw/tt/points_of_departure/">point of departure</a>.</p>
<p>What is it that finally breaks this attachment?</p>
<p>It's an embarrassing confession:&nbsp; It came from a science-fiction story I was trying to write.&nbsp; (No, you can't see it; it's not done.) The story involved a non-cognitive non-evolutionary optimization process; something like an <a href="https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">Outcome Pump</a>. Not intelligence, but a cross-temporal physical effect&mdash;that is, I was imagining it as a physical effect&mdash;that narrowly constrained the space of possible outcomes.&nbsp; (I can't tell you any more than that; it would be a spoiler, if I ever finished the story.&nbsp; Just see the post on <a href="https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/">Outcome Pumps</a>.) It was "just a story", and so I was free to play with the idea and elaborate it out logically:&nbsp; C was constrained to happen, therefore B (in the past) was constrained to happen, therefore A (which led to B) was constrained to happen.</p>
<p>Drawing a line through one point is generally held to be dangerous. Two points make a dichotomy; you imagine them opposed to one another. But when you've got three different points&mdash;<em>that's</em> when you're forced to wake up and generalize.</p>
<p>Now I had three points:&nbsp; Human intelligence, natural selection, and my fictional plot device.</p>
<p>And so that was the point at which I generalized <a href="https://www.lesswrong.com/lw/tx/optimization/">the notion of an optimization process</a>, of a process that squeezes the future into a narrow region of the possible.</p>
<p>This may seem like an obvious point, if you've been following <em>Overcoming Bias</em> this whole time; but if you look at Shane Legg's collection of <a href="https://web.archive.org/web/20100407061055/http://www.vetta.org/definitions-of-intelligence/">71 definitions of intelligence</a>, you'll see that "<a href="https://www.lesswrong.com/lw/tx/optimization/">squeezing the future into a constrained region</a>" is a less obvious reply than it seems.</p>
<p>Many of the definitions of "intelligence" by AI researchers, do talk about "solving problems" or "achieving goals".&nbsp; But from the viewpoint of past Eliezers, at least, it is only hindsight that makes this the same thing as "squeezing the future".</p>
<p>A <em>goal</em> is a mentalistic object; electrons have no goals, and solve no problems either.&nbsp; When a human imagines a goal, they imagine an agent imbued with wanting-ness&mdash;it's still <a href="https://www.lesswrong.com/lw/te/three_fallacies_of_teleology/">empathic language</a>.</p>
<p>You can espouse the notion that intelligence is about "achieving goals"&mdash;and then turn right around and argue about whether some "goals" are better than others&mdash;or talk about the wisdom required to judge between goals themselves&mdash;or talk about a system deliberately modifying its goals&mdash;or talk about the free will needed to <em>choose</em> plans that achieve goals&mdash;or talk about an AI realizing that its goals aren't what the programmers really meant to ask for.&nbsp; If you imagine something that squeezes the future into a narrow region of the possible, like an Outcome Pump, those seemingly sensible statements somehow don't translate.</p>
<p>So for me at least, seeing through the word "mind", to a physical process that would, just by naturally running, just by obeying the laws of physics, end up squeezing its future into a narrow region, was a naturalistic enlightenment over and above the notion of an agent trying to achieve its goals.</p>
<p>It was like falling out of a deep pit, falling into the ordinary world, strained cognitive tensions relaxing into unforced simplicity, confusion turning to smoke and drifting away.&nbsp; I saw the <em>work performed</em> by intelligence; <em>smart</em> was no longer a property, but an engine.&nbsp; Like a knot in time, echoing the outer part of the universe in the inner part, and thereby steering it.&nbsp; I even saw, in a flash of the same enlightenment, that <a href="https://www.lesswrong.com/lw/o5/the_second_law_of_thermodynamics_and_engines_of/">a mind had to output waste heat in order to obey the laws of thermodynamics</a>.</p>
<p>Previously, Eliezer<sub>2001</sub> had talked about Friendly AI as something you should do just to be sure&mdash;if you didn't know whether AI design X was going to be Friendly, then you really ought to go with AI design Y that you did know would be Friendly.&nbsp; But Eliezer<sub>2001</sub> didn't think he <em>knew</em> whether you could <em>actually</em> have a superintelligence that turned its future light cone into paperclips.</p>
<p>Now, though, I could <em>see</em> it&mdash;the pulse of the optimization process, sensory information surging in, motor instructions surging out, steering the future.&nbsp; In the middle, the model that linked up possible actions to possible outcomes, and the utility function over the outcomes.&nbsp; Put in the corresponding utility function, and the result would be an optimizer that would steer the future anywhere.</p>
<p>Up until that point, I'd never quite admitted to myself that Eliezer<sub>1997</sub>'s AI goal system design would definitely, no two ways about it, pointlessly wipe out the human species.&nbsp; Now, however, I looked back, and I could finally see <em>what my old design really did</em>, to the extent it was coherent enough to be talked about.&nbsp; Roughly, it would have converted its future light cone into generic tools&mdash;computers without programs to run, stored energy without a use...</p>
<p>...how on Earth had I, the fine and practiced rationalist, how on Earth had I managed to miss something that obvious, for six damned years?</p>
<p>That was the point at which I awoke clear-headed, and remembered; and thought, with a certain amount of embarrassment:&nbsp; <em>I've been stupid.</em></p>
<p>To be continued.</p>