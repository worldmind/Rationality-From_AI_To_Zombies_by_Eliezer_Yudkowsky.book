<p>My father used to say that if the present system had been in place a hundred years ago, automobiles would have been outlawed to protect the saddle industry.</p>
<p>One of my major childhood influences was reading Jerry Pournelle's <em>A Step Farther Out</em>, at the age of nine.&nbsp; It was Pournelle's reply to Paul Ehrlich and the Club of Rome, who were saying, in the 1960s and 1970s, that the Earth was running out of resources and massive famines were only years away.&nbsp; It was a reply to Jeremy Rifkin's so-called fourth law of thermodynamics; it was a reply to all the people scared of nuclear power and trying to regulate it into oblivion.</p>
<p>I grew up in a world where the lines of demarcation between the Good Guys and the Bad Guys were pretty clear; not an apocalyptic final battle, but a battle that had to be fought over and over again, a battle where you could see the historical echoes going back to the Industrial Revolution, and where you could assemble the historical evidence about the actual outcomes.</p>
<p>On one side were the scientists and engineers who'd driven all the standard-of-living increases since the Dark Ages, whose work supported luxuries like democracy, an educated populace, a middle class, the outlawing of slavery.</p>
<p>On the other side, those who had once opposed smallpox vaccinations, anesthetics during childbirth, steam engines, and heliocentrism:&nbsp; The theologians calling for a return to a perfect age that never existed, the elderly white male politicians set in their ways, the special interest groups who stood to lose, and the many to whom science was a closed book, fearing what they couldn't understand.</p>
<p>And trying to play the middle, the pretenders to <a href="https://www.lesswrong.com/lw/k8/how_to_seem_and_be_deep/">Deep Wisdom</a>, uttering <a href="https://www.lesswrong.com/lw/k5/cached_thoughts/">cached thoughts</a> about how technology benefits humanity but only when it was properly regulated&mdash;claiming in defiance of brute historical fact that science of itself was neither good nor evil&mdash;setting up solemn-looking bureaucratic committees to make an ostentatious display of their caution&mdash;and waiting for their <a href="https://www.lesswrong.com/lw/jb/applause_lights/">applause</a>.&nbsp; As if the truth were always a compromise.&nbsp; And as if anyone could really see that far ahead.&nbsp; Would humanity have done better if there'd been a sincere, concerned, public debate on the adoption of fire, and commitees set up to oversee its use?</p>
<p><a id="more"></a></p>
<p>When I entered into the problem, I started out allergized against anything that pattern-matched "Ah, but technology has risks as well as benefits, litte one."&nbsp; The presumption-of-guilt was that you were either trying to collect some cheap applause, or covertly trying to regulate the technology into oblivion.&nbsp; And either way, ignoring the historical record immensely in <em>favor</em> of technologies that people had once worried about.</p>
<p>Today, Robin Hanson raised the topic of <a href="http://www.overcomingbias.com/2008/09/deafening-silen.html">slow FDA approval of drugs approved in other countries</a>.&nbsp; Someone in the comments <a href="http://www.overcomingbias.com/2008/09/deafening-silen.html#comment-130860808">pointed out</a> that Thalidomide was sold in 50 countries under 40 names, but that only a small amount was given away in the US, so that there were 10,000 malformed children born globally, but only 17 children in the US.</p>
<p>But how many people have died because of the slow approval in the US, of drugs more quickly approved in other countries&mdash;all the drugs that <em>didn't</em> go wrong?&nbsp; And I ask that question because it's what you can try to collect statistics about&mdash;this says nothing about all the drugs that were never <em>developed</em> because the approval process is too long and costly.&nbsp; According to <a href="http://www.fdareview.org/harm.shtml">this source</a>, the FDA's longer approval process prevents 5,000 casualties per year by screening off medications found to be harmful, and causes at least 20,000-120,000 casualties per year just by delaying approval of those beneficial medications that are still developed and eventually approved.</p>
<p>So there really is a reason to be allergic to people who go around saying, "Ah, but technology has risks as well as benefits".&nbsp; There's a historical record showing over-conservativeness, the many silent deaths of regulation being outweighed by a few visible deaths of nonregulation.&nbsp; If you're <em>really</em> playing the middle, why not say, "Ah, but technology has benefits as well as risks"?</p>
<p>Well, and this isn't such a bad description of the Bad Guys.&nbsp; (Except that it ought to be emphasized a bit harder that these aren't <a href="https://www.lesswrong.com/lw/i0/are_your_enemies_innately_evil/">evil mutants</a> but standard human beings acting under a different worldview-gestalt that puts them in the right; some of them will inevitably be more competent than others, and competence counts for a lot.)&nbsp; Even looking back, I don't think my childhood technophilia was too wrong about what constituted a Bad Guy and what was the key mistake.&nbsp; But it's always a <em>lot</em> easier to say what <em>not</em> to do, than to get it <em>right</em>.&nbsp; And one of my fundamental flaws, back then, was thinking that, if you tried as hard as you could to avoid everything the Bad Guys were doing, that made you a Good Guy.</p>
<p>Particularly damaging, I think, was the bad example set by the pretenders to Deep Wisdom trying to stake out a middle way; smiling condescendingly at technophiles and technophobes alike, and calling them both immature.&nbsp; Truly this is <em>a </em>wrong way; and in fact, the notion of trying to stake out a middle way generally, is usually wrong; the Right Way is not a compromise with anything, it is the clean manifestation of its own criteria.</p>
<p>But that made it more difficult for the young Eliezer to depart from the charge-straight-ahead verdict, because <em>any</em> departure felt like joining the pretenders to Deep Wisdom.</p>
<p>The first crack in my childhood technophilia appeared in, I think, 1997 or 1998, at the point where I noticed my fellow technophiles saying foolish things about how molecular nanotechnology would be an easy problem to manage.&nbsp; (As you may be noticing yet again, the young Eliezer was driven to a tremendous extent by his ability to find flaws&mdash;I even had a personal philosophy of why that sort of thing was a good idea.)</p>
<p>The nanotech stuff would be a separate post, and maybe one that should go on a different blog.&nbsp; But there was a debate going on about molecular nanotechnology, and whether offense would be asymmetrically easier than defense.&nbsp; And there were people arguing that defense would be easy.&nbsp; In the domain of <em>nanotech,</em> for Ghu's sake, programmable matter, when we can't even seem to get the security problem solved for computer networks where we can observe and control every one and zero.&nbsp; People were talking about unassailable diamondoid walls.&nbsp; I observed that diamond doesn't stand off a nuclear weapon, that offense has had defense beat since 1945 and nanotech didn't look likely to change that.</p>
<p>And by the time that debate was over, it seems that the young Eliezer&mdash;caught up in the heat of argument&mdash;had managed to notice, for the first time, that the survival of Earth-originating intelligent life stood at risk.</p>
<p>It seems so strange, looking back, to think that there was a time when I thought that only individual lives were at stake in the future.&nbsp; What a profoundly friendlier world that was to live in... though it's not as if I were thinking that at the time.&nbsp; I didn't <em>reject</em> the possibility so much as <em>manage to never see it in the first place.</em>&nbsp; Once the topic actually came up, I saw it.&nbsp; I don't really remember how that trick worked.&nbsp; There's a reason why I refer to my past self in the third person.</p>
<p>It may sound like Eliezer<sub>1998</sub> was a complete idiot, but that would be a comfortable out, in a way; the truth is scarier.&nbsp; Eliezer<sub>1998</sub> was a sharp Traditional Rationalist, as such things went.&nbsp; I knew hypotheses had to be <a href="https://www.lesswrong.com/lw/qd/science_isnt_strict_enough/">testable</a>, I knew that <a href="https://www.lesswrong.com/lw/ju/rationalization/">rationalization</a> was not a permitted mental operation, I knew how to play <a href="https://www.lesswrong.com/lw/nu/taboo_your_words/">Rationalist's Taboo</a>, I was obsessed with self-awareness... I didn't quite understand the concept of "<a href="https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/">mysterious answers</a>"... and no Bayes or Kahneman at all.&nbsp; But a sharp Traditional Rationalist, far above average...&nbsp; So what?&nbsp; Nature isn't grading us on a curve.&nbsp; One step of departure from the Way, one shove of undue influence on your thought processes, can repeal all other protections.</p>
<p>One of the chief lessons I derive from looking back at my personal history is that it's no wonder that, out there in the real world, a lot of people think that "intelligence isn't everything", or that rationalists don't do better in real life.&nbsp; A little rationality, or even a lot of rationality, doesn't pass the astronomically high barrier required for things to actually start <em>working.</em></p>
<p>Let not my misinterpretation of the Right Way be blamed on Jerry Pournelle, my father, or science fiction generally.&nbsp; I think the young Eliezer's personality imposed quite a bit of selectivity on which parts of their teachings made it through.&nbsp; It's not as if Pournelle didn't say:&nbsp; <em>The rules change once you leave Earth, the cradle; if you're careless sealing your pressure suit just once, you die.</em>&nbsp; He said it quite a bit.&nbsp; But the words didn't really seem important, because that was something that happened to third-party characters in the novels&mdash;the main character didn't usually die halfway through, for some reason.</p>
<p>What was the lens through which I filtered these teachings?&nbsp; Hope. Optimism.&nbsp; Looking forward to a brighter future.&nbsp; That was the fundamental meaning of <em>A Step Farther Out </em>unto me, the lesson I took in contrast to the Sierra Club's doom-and-gloom.&nbsp; On one side was <em>rationality and hope,</em> the other, <em>ignorance and despair</em>.</p>
<p>Some teenagers think they're immortal and ride motorcycles.&nbsp; I was under no such illusion and quite reluctant to learn to drive, considering how unsafe those hurtling hunks of metal looked.&nbsp; But there was something more important to me than my own life:&nbsp; The Future.&nbsp; And I acted as if <em>that</em> was immortal.&nbsp; Lives could be lost, but not the Future.</p>
<p>And when I noticed that nanotechnology really <em>was</em> going to be a potentially extinction-level challenge?</p>
<p>The young Eliezer thought, explicitly, "Good heavens, how did I fail to notice this thing that should have been obvious?&nbsp; I must have been too emotionally attached to the benefits I expected from the technology; I must have flinched away from the thought of human extinction."</p>
<p>And then...</p>
<p>I didn't declare a Halt, Melt, and Catch Fire.&nbsp; I didn't rethink all the conclusions that I'd developed with my prior attitude.&nbsp; I just managed to integrate it into my worldview, <em>somehow,</em> with a minimum of propagated changes.&nbsp; Old ideas and plans were challenged, but my mind found reasons to keep them.&nbsp; There was no systemic breakdown, unfortunately.</p>
<p>Most notably, I decided that we had to run full steam ahead on AI, so as to develop it before nanotechnology.&nbsp; Just like I'd been <em>originally</em> planning to do, but now, with a <em>different reason.</em></p>
<p>I guess that's what most human beings are like, isn't it?&nbsp; Traditional Rationality wasn't enough to change that.</p>
<p>But there did come a time when I fully realized my mistake.&nbsp; It just took a stronger boot to the head.&nbsp; To be continued.</p>