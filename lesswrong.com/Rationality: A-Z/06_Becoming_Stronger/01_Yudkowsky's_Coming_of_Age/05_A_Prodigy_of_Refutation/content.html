<p><a href="/lw/ty/my_childhood_death_spiral/">My Childhood Death Spiral</a> described the core momentum carrying me into my mistake, an <a href="/lw/lm/affective_death_spirals/">affective death spiral</a> around something that Eliezer<sub>1996</sub> called "intelligence".&nbsp; I was also a <a href="/lw/u0/raised_in_technophilia/">technophile</a>, pre-allergized against fearing the future.&nbsp; And I'd read a lot of science fiction built around personhood ethics&mdash;in which fear of the Alien puts humanity-at-large in the position of the bad guys, mistreating aliens or sentient AIs because they "aren't human".</p>
<p>That's part of the ethos you acquire from science fiction&mdash;to define your in-group, your tribe, appropriately broadly.&nbsp; Hence my email address, sentience@pobox.com.</p>
<p>So Eliezer<sub>1996</sub> is out to build superintelligence, for the good of humanity and all sentient life.</p>
<p>At first, I think, the question of whether a superintelligence will/could be good/evil didn't really occur to me as a separate topic of discussion.&nbsp; Just the standard intuition of, "Surely no supermind would be stupid enough to turn the galaxy into paperclips; surely, being so intelligent, it will also know what's <em>right</em> far better than a human being could."</p>
<p>Until I introduced myself and my quest to a transhumanist mailing list, and got back responses along the general lines of (from memory):</p>
<p><a id="more"></a></p>
<blockquote>
<p>Morality is arbitrary&mdash;if you say that something is good or bad, you can't be right or wrong about that.&nbsp; A superintelligence would form its own morality.</p>
<p>Everyone ultimately looks after their own self-interest.&nbsp; A superintelligence would be no different; it would just seize all the resources.</p>
<p>Personally, I'm a human, so I'm in favor of humans, not Artificial Intelligences.&nbsp; I don't think we should develop this technology. Instead we should develop the technology to upload humans first.</p>
<p>No one should develop an AI without a control system that watches it and makes sure it can't do anything bad.</p>
</blockquote>
<p>Well, <em>that's</em> all obviously wrong, thinks Eliezer<sub>1996</sub>, and he proceeded to kick his opponents' arguments to pieces.&nbsp; (I've mostly done this in other blog posts, and anything remaining is left as an exercise to the reader.)</p>
<p>It's not that Eliezer<sub>1996</sub>&nbsp; explicitly reasoned, "The world's stupidest man says the sun is shining, <em>therefore</em> it is dark out."&nbsp; But Eliezer<sub>1996</sub> was a Traditional Rationalist; he had been inculcated with the metaphor of science as a <em>fair fight</em> between sides who take on different positions, stripped of mere violence and other such exercises of political muscle, so that, ideally, the side with the best arguments can win.</p>
<p>It's easier to say where someone else's argument is wrong, then to get the fact of the matter right; and Eliezer<sub>1996</sub> was <em>very skilled</em> at finding flaws.&nbsp; (So am I.&nbsp; It's not as if you can solve <a href="/lw/he/knowing_about_biases_can_hurt_people/">the danger of that power</a> by refusing to care about flaws.)&nbsp; From Eliezer<sub>1996</sub>'s perspective, it seemed to him that his chosen side was <em>winning the fight</em>&mdash;that he was formulating better arguments than his opponents&mdash;so why would he switch sides?</p>
<p>Therefore is it <a href="http://yudkowsky.net/virtues/">written</a>:&nbsp; "Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities.&nbsp; But it is useless to be superior:&nbsp; Life is not graded on a curve.&nbsp; The best physicist in ancient Greece could not calculate the path of a falling apple.&nbsp; There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse."</p>
<p>You cannot rely on <em>anyone</em> else to argue you out of your mistakes; you cannot rely on <em>anyone</em> else to save you; you and <em>only</em> you are obligated to find the flaws in your positions; if you put that burden down, don't expect anyone else to pick it up.&nbsp; And I wonder if that advice will turn out not to help most people, until they've personally blown off their own foot, saying to themselves all the while, <em>correctly</em>, "Clearly I'm winning this argument."</p>
<p>Today I try not to take any human being as my opponent.&nbsp; That just leads to overconfidence.&nbsp; It is Nature that I am facing off against, who does not match Her problems to your skill, who is not obliged to offer you a fair chance to win in return for a diligent effort, who does not care if you are the best who ever lived, if you are not good <em>enough.</em></p>
<p>But return to 1996.&nbsp; Eliezer<sub>1996</sub> is going with the basic intuition of "Surely a superintelligence will know better than we could what is <em>right,</em>" and offhandedly knocking down various arguments brought against his position.&nbsp; He was skillful in that way, you see.&nbsp; He even had a personal philosophy of why it was wise to look for flaws in things, and so on.</p>
<p>I don't mean to say it as an excuse, that no one who argued against Eliezer<sub>1996</sub>, actually presented him with the <a href="/lw/of/dissolving_the_question/">dissolution</a> of the mystery&mdash;the full reduction of morality that analyzes all his cognitive processes debating "morality", a step-by-step walkthrough of the algorithms that make morality feel to him like a fact.&nbsp; Consider it rather as an indictment, a measure of Eliezer<sub>1996</sub>'s level, that he would have needed the full solution given to him, in order to present him with an argument that he could <em>not</em> refute.</p>
<p>The few philosophers present, did not extract him from his difficulties.&nbsp; It's not as if a philosopher will say, "Sorry, morality is understood, it is a settled issue in cognitive science and philosophy, and your viewpoint is simply wrong."&nbsp; The nature of morality is still an open question in philosophy, the debate is still going on.&nbsp; A philosopher will feel obligated to present you with a list of classic arguments on all sides; most of which Eliezer<sub>1996</sub> is quite intelligent enough to knock down, and so he concludes that philosophy is a wasteland.</p>
<p>But wait.&nbsp; It gets worse.</p>
<p>I don't recall exactly when&mdash;it might have been 1997&mdash;but the younger me, let's call him Eliezer<sub>1997</sub>, set out to argue<em> inescapably</em> that creating superintelligence is the right thing to do.&nbsp; To be continued.</p>