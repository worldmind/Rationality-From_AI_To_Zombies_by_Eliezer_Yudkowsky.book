<p>When we last left Eliezer<sub>2000</sub>, he was just beginning to investigate the question of how to inscribe a morality into an AI.&nbsp; His reasons for doing this don't matter at all, except insofar as they happen to historically demonstrate the importance of perfectionism.&nbsp; If you practice something, you may get better at it; if you investigate something, you may find out about it; the only thing that matters is that Eliezer<sub>2000</sub> is, in fact, focusing his full-time energies on thinking technically about AI morality; rather than, as previously, finding an justification for not spending his time this way.&nbsp; In the end, this is all that turns out to matter.</p>
<p>But as our story begins&mdash;as the sky lightens to gray and the tip of the sun peeks over the horizon&mdash;Eliezer<sub>2001</sub> hasn't yet admitted that Eliezer<sub>1997</sub> was <em>mistaken</em> in any important sense.&nbsp; He's just making Eliezer<sub>1997</sub>'s strategy <em>even better</em> by including a <em>contingency</em> plan for "the unlikely event that life turns out to be meaningless"...</p>
<p>...which means that Eliezer<sub>2001</sub> now has a <a href="https://www.lesswrong.com/lw/o4/leave_a_line_of_retreat/">line of retreat</a> away from his mistake.</p>
<p>I don't just mean that Eliezer<sub>2001</sub> can say "Friendly AI is a contingency plan", rather than <a href="https://www.lesswrong.com/lw/i9/the_importance_of_saying_oops/">screaming "OOPS!"</a></p>
<p>I mean that Eliezer<sub>2001</sub> now actually <em>has</em> a contingency plan.&nbsp; If Eliezer<sub>2001</sub> starts to doubt his 1997 metaethics, the Singularity has a fallback strategy, namely Friendly AI.&nbsp; Eliezer<sub>2001</sub> can <a href="https://www.lesswrong.com/lw/sk/changing_your_metaethics/">question his metaethics without it signaling the end of the world</a>.</p>
<p>And his gradient has been smoothed; he can admit a 10% chance of having previously been wrong, then a 20% chance.&nbsp; He doesn't have to cough out his whole mistake in one huge lump.</p>
<p>If you think this sounds like Eliezer<sub>2001</sub> is <a href="https://www.lesswrong.com/lw/qt/class_project/">too slow</a>, I <a href="https://www.lesswrong.com/lw/i9/the_importance_of_saying_oops/">quite agree</a>.</p>
<p><a id="more"></a></p>
<p>Eliezer<sub>1996-2000</sub>'s strategies had been formed in the total absence of "Friendly AI" as a consideration.&nbsp; The whole idea was to get a superintelligence, <em>any</em> superintelligence, as fast as possible&mdash;codelet soup, ad-hoc heuristics, evolutionary programming, open-source, anything that looked like it might work&mdash;preferably all approaches simultaneously in a Manhattan Project.&nbsp; ("All parents did the things they tell their children not to do.&nbsp; That's how they know to tell them not to do it."&nbsp; John Moore, <em>Slay and Rescue</em>.)&nbsp; It's not as if adding one more approach could <em>hurt.</em></p>
<p>His attitudes toward technological progress have been formed&mdash;or more accurately, preserved from <a href="https://www.lesswrong.com/lw/u0/raised_in_technophilia/">childhood-absorbed technophilia</a>&mdash;around the assumption that any/all movement toward superintelligence is a <a href="https://www.lesswrong.com/lw/ty/my_childhood_death_spiral/">pure good</a> <a href="https://www.lesswrong.com/lw/gz/policy_debates_should_not_appear_onesided/">without a hint of danger</a>.</p>
<p>Looking back, what Eliezer<sub>2001</sub>&nbsp; <em>needed</em> to do at this point was declare an HMC event&mdash;Halt, Melt, and Catch Fire.&nbsp; One of the foundational assumptions on which everything else has been built, has been revealed as flawed.&nbsp; This calls for a mental brake to a full stop: <a href="https://www.lesswrong.com/lw/s3/the_genetic_fallacy/">take your weight off all beliefs built on the wrong assumption</a>, do your best to rethink everything from scratch.&nbsp; This is an art I need to write more about&mdash;it's akin to the convulsive effort required to seriously clean house, after an adult religionist notices for the first time that God doesn't exist.</p>
<p>But what Eliezer<sub>2001</sub> actually did was <a href="https://www.lesswrong.com/lw/ik/one_argument_against_an_army/">rehearse</a> his previous technophilic arguments for why it's difficult to ban or governmentally control new technologies&mdash;the standard arguments against "relinquishment".</p>
<p>It does seem even to my modern self, that all those awful consequences which technophiles argue to follow from various kinds of government regulation, are more or less correct&mdash;it's much easier to say what someone is doing wrong, than to say the way that is right.&nbsp; My modern viewpoint hasn't shifted to think that technophiles are wrong about the downsides of technophobia; but I do tend to be a lot more sympathetic to what technophobes say about the downsides of technophilia.&nbsp; What previous Eliezers said about the difficulties of, e.g., the government doing anything sensible about Friendly AI, still seems pretty true.&nbsp; It's just that a lot of his hopes for science, or private industry, etc., now seem equally wrongheaded.</p>
<p>Still, let's not get into the details of the <a href="http://www.acceleratingfuture.com/steven/?p=62">technovolatile</a> viewpoint.&nbsp; Eliezer<sub>2001</sub> has just tossed a major foundational assumption&mdash;that AI can't be dangerous, unlike other technologies&mdash;out the window.&nbsp; You would intuitively suspect that this should have some kind of large effect on his strategy.</p>
<p>Well, Eliezer<sub>2001</sub> did at least give up on his 1999 idea of an open-source AI Manhattan Project using self-modifying heuristic soup, but overall...</p>
<p>Overall, he'd previously wanted to charge in, guns blazing, immediately using his best idea at the time; and afterward he still wanted to charge in, guns blazing.&nbsp; He didn't say, "I don't know how to do this."&nbsp; He didn't say, "I need better knowledge."&nbsp; He didn't say, "This project is not yet ready to start coding."&nbsp; It was still all, "The clock is ticking, gotta move now!&nbsp; The Singularity Institute will start coding as soon as it's got enough money!"</p>
<p>Before, he'd wanted to focus as much scientific effort as possible with full information-sharing, and afterward he still thought in those terms.&nbsp; Scientific secrecy = bad guy, openness = good guy.&nbsp; (Eliezer<sub>2001</sub> hadn't read up on the Manhattan Project and wasn't familiar with the similar argument that Leo Szilard had with Enrico Fermi.)</p>
<p>That's the problem with converting one big "Oops!" into a gradient of shifting probability.&nbsp; It means there isn't a single watershed moment&mdash;a visible huge impact&mdash;to hint that equally huge changes might be in order.</p>
<p>Instead, there are all these little opinion shifts... that give you a chance to repair the <em>arguments</em> for your strategies; to shift the justification a little, but keep the "basic idea" in place.&nbsp; Small shocks that the system can absorb without cracking, because each time, it gets a chance to go back and repair itself.&nbsp; It's just that in the domain of rationality, cracking = good, repair = bad.&nbsp; In the art of rationality it's far <a href="https://www.lesswrong.com/lw/i9/the_importance_of_saying_oops/">more efficient</a> to admit one huge mistake, than to admit lots of little mistakes.</p>
<p>There's some kind of instinct humans have, I think, to preserve their former strategies and plans, so that they aren't constantly thrashing around and wasting resources; and of course an instinct to preserve any position that we have publicly argued for, so that we don't suffer the humiliation of being wrong.&nbsp; And though the younger Eliezer has striven for rationality for many years, he is not immune to these impulses; they waft gentle influences on his thoughts, and this, unfortunately, is more than enough damage.</p>
<p>Even in 2002, the earlier Eliezer isn't yet <em>sure</em> that Eliezer<sub>1997</sub>'s plan <em>couldn't possibly</em> have worked.&nbsp; It <em>might</em> have gone right.&nbsp; You never know, right?</p>
<p>But there came a time when it all fell crashing down.&nbsp; To be continued.</p>
