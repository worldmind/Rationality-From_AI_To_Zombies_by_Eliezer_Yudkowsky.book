<p>What is so <em>terrifying</em> about the idea that not every possible mind might agree with us, even in principle?</p>
<p>For some folks, nothing&mdash;it doesn't bother them in the slightest<span style="font-style: normal;">. And for some of </span><em>those</em><span style="font-style: normal;"> folks, the </span><em>reason</em><span style="font-style: normal;"> it doesn't bother them is that they don't have strong intuitions about standards and truths that go beyond personal whims.&nbsp; If they say the sky is blue, or that murder is wrong, that's just their personal opinion; and that someone else might have a different opinion doesn't surprise them.</span></p>
<p><span style="font-style: normal;">For other folks, a disagreement that persists even </span><em>in principle</em><span style="font-style: normal;"> is something they can't accept.&nbsp; And for some of </span><em>those</em><span style="font-style: normal;"> folks, the </span><em>reason</em><span style="font-style: normal;"> it bothers them, is that it seems to them that if you allow that some people cannot be persuaded </span><em>even in principle</em><span style="font-style: normal;"> that the sky is blue, then you're conceding that "the sky is blue" is merely an </span><em>arbitrary</em><span style="font-style: normal;"> personal opinion.</span></p>
<p><span style="font-style: normal;"><a href="https://www.lesswrong.com/lw/rm/the_design_space_of_mindsingeneral/">Yesterday</a>, I proposed that you should resist the temptation to generalize over all of mind design space.&nbsp; If we restrict ourselves to minds specifiable in a trillion bits or less, then each </span><em>universal</em><span style="font-style: normal;"> generalization "All minds m: X(m)" has two to the trillionth chances to be false, while each </span><em>existential</em><span style="font-style: normal;"> generalization "Exists mind m: X(m)" has two to the trillionth chances to be true.</span></p>
<p style="font-style: normal;">This would seem to argue that for every argument A, howsoever convincing it may seem to us, there exists at least one possible mind that doesn't buy it.</p>
<p><span style="font-style: normal;">And the surprise and/or horror of this prospect (for some) has a great deal to do, I think, with the intuition of the <a href="https://www.lesswrong.com/lw/rf/ghosts_in_the_machine/">ghost-in-the-machine</a>&mdash;a ghost with some irreducible core that any </span><em>truly valid</em><span style="font-style: normal;"> argument will convince.</span></p>
<p><a id="more"></a></p>
<p>I have <a href="https://www.lesswrong.com/lw/rf/ghosts_in_the_machine/">previously spoken</a> of the intuition whereby people <a href="https://www.lesswrong.com/lw/rj/surface_analogies_and_deep_causes/">map</a> <em>programming a computer</em>, onto <em>instructing a human servant</em>, so that the computer might rebel against its code&mdash;or perhaps look over the code, decide it is not reasonable, and hand it back.</p>
<p style="font-style: normal;">If there were a ghost in the machine and the ghost contained an irreducible core of reasonableness, above which any mere code was only a suggestion, then there might be universal arguments.&nbsp; Even if the ghost was initially handed code-suggestions that contradicted the Universal Argument, then when we finally did expose the ghost to the Universal Argument&mdash;or the ghost could discover the Universal Argument on its own, that's also a popular concept&mdash;the ghost would just override its own, mistaken source code.</p>
<p><span style="font-style: normal;">But as the student programmer once said, "</span>I get the feeling that the computer just skips over all the comments."<span style="font-style: normal;">&nbsp; The code is not given to the AI; the code </span><em>is</em><span style="font-style: normal;"> the AI.</span></p>
<p><span style="font-style: normal;">If you switch to the physical perspective, then the notion of a Universal Argument seems noticeably unphysical.&nbsp; If there's a physical system that at time T, after being exposed to argument E, does X, then there ought to be another physical system that at time T, after being exposed to environment E, does Y.&nbsp; Any thought has to be implemented </span><em>somewhere</em><span style="font-style: normal;">, in a physical system; any belief, any conclusion, any decision, any motor output.&nbsp; For every lawful causal system that zigs at a set of points, you should be able to specify another causal system that lawfully zags at the same points.</span></p>
<p><span style="font-style: normal;">Let's say there's a mind with a transistor that outputs +3 volts at time T, indicating that it has just assented to some persuasive argument.&nbsp; Then we can build a highly similar physical cognitive system with a tiny little trapdoor underneath the transistor containing a little grey man who climbs out at time T and sets that transistor's output to&mdash;3 volts, indicating non-assent.&nbsp; Nothing acausal about that; the little grey man is there because we built him in.&nbsp; The notion of an argument that convinces </span><em>any</em><span style="font-style: normal;"> mind seems to involve a little blue woman who was </span><em>never</em><span style="font-style: normal;"> built into the system, who climbs out of literally </span><em>nowhere,</em><span style="font-style: normal;"> and strangles the little grey man, because that transistor has just </span><em>got</em><span style="font-style: normal;"> to output +3 volts:&nbsp; It's such a </span><em>compelling argument,</em><span style="font-style: normal;"> you see.</span></p>
<p style="font-style: normal;">But compulsion is not a property of arguments, it is a <a href="https://www.lesswrong.com/lw/oi/mind_projection_fallacy/">property of minds</a> that process arguments.</p>
<p><span style="font-style: normal;">So the reason I'm arguing against the ghost, isn't </span><em>just</em><span style="font-style: normal;"> to make the point that (1) Friendly AI has to be explicitly programmed and (2) the laws of physics do not forbid Friendly AI. (Though of course I take a certain interest in establishing this.)</span></p>
<p><span style="font-style: normal;">I also wish to establish the notion of a mind as a </span><em>causal, lawful, physical system</em><span style="font-style: normal;"> in which there </span><em>is no</em><span style="font-style: normal;"> irreducible central ghost that looks over the neurons / code and decides whether they are good suggestions.</span></p>
<p><span style="font-style: normal;">(There is a concept in Friendly AI of </span><em>deliberately</em><span style="font-style: normal;"> programming an FAI to review its own source code and possibly hand it back to the programmers.&nbsp; But the mind that reviews is not irreducible, it is just the mind that you created.&nbsp; The FAI is renormalizing itself </span><em>however it was designed to do so;</em><span style="font-style: normal;"> there is nothing acausal reaching in from outside.&nbsp; A bootstrap, not a skyhook.)</span></p>
<p style="font-style: normal;">All this echoes back to the <a href="https://www.lesswrong.com/lw/k2/a_priori/">discussion</a>, a good deal earlier, of a Bayesian's "arbitrary" <a href="https://www.lesswrong.com/lw/hk/priors_as_mathematical_objects/">priors</a>.&nbsp; If you show me one Bayesian who draws 4 red balls and 1 white ball from a barrel, and who assigns probability 5/7 to obtaining a red ball on the next occasion (by Laplace's Rule of Succession), then I can show you <a href="https://www.lesswrong.com/lw/hk/priors_as_mathematical_objects/">another mind</a> which obeys Bayes's Rule to conclude a 2/7 probability of obtaining red on the next occasion&mdash;corresponding to a different prior belief about the barrel, but, perhaps, a less "reasonable" one.</p>
<p style="font-style: normal;">Many philosophers are convinced that because you can in-principle construct a prior that updates to any given conclusion on a stream of evidence, therefore, Bayesian reasoning must be "arbitrary", and the whole schema of Bayesianism flawed, because it relies on "unjustifiable" assumptions, and indeed "unscientific", because you cannot force any possible journal editor in mindspace to agree with you.</p>
<p style="font-style: normal;">And this (I then replied) relies on the notion that by unwinding all arguments and their justifications, you can obtain an <a href="https://www.lesswrong.com/lw/k2/a_priori/">ideal philosophy student of perfect emptiness</a>, to be convinced by a line of reasoning that begins from absolutely no assumptions.</p>
<p style="font-style: normal;">But who is this ideal philosopher of perfect emptiness?&nbsp; Why, it is just the irreducible core of the ghost!</p>
<p><span style="font-style: normal;">And that is why (I went on to say) the result of trying to remove all assumptions from a mind, and unwind to the perfect absence of any prior, is not an ideal philosopher of perfect emptiness, but a rock.&nbsp; What is left of a mind after you remove the source code?&nbsp; Not the ghost who looks over the source code, but simply... no ghost.</span></p>
<p><span style="font-style: normal;">So&mdash;and I shall take up this theme again later&mdash;wherever you are to locate your notions of </span><em>validity</em><span style="font-style: normal;"> or </span><em>worth </em><span style="font-style: normal;">or </span><em>rationality</em><span style="font-style: normal;"> or </span><em>justification</em><span style="font-style: normal;"> or even </span><em>objectivity,</em><span style="font-style: normal;"> it cannot rely on an argument that is </span><em>universally compelling to all physically possible minds.</em></p>
<p><span style="font-style: normal;">Nor can you ground validity in a sequence of justifications that, beginning from nothing, persuades a perfect emptiness.</span></p>
<p><span style="font-style: normal;">Oh, there might be argument sequences that would compel any neurologically intact </span><em>human</em><span style="font-style: normal;">&mdash;like the argument I use to make people <a href="https://yudkowsky.net/singularity/aibox/">let the AI out of the box</a></span><sup><span style="font-style: normal;">1</span></sup><span style="font-style: normal;">&mdash;but that is hardly the same thing from a philosophical perspective.</span></p>
<p style="font-style: normal;">The first great failure of those who try to consider Friendly AI, is the One Great Moral Principle That Is All We Need To Program&mdash;aka the <a href="https://www.lesswrong.com/lw/lq/fake_utility_functions/">fake utility function</a>&mdash;and of this I have already spoken.</p>
<p><span style="font-style: normal;">But the even worse failure is the One Great Moral Principle We Don't Even </span><em>Need</em><span style="font-style: normal;"> To Program Because Any AI Must Inevitably Conclude It.&nbsp; This notion exerts a terrifying unhealthy fascination on those who spontaneously reinvent it; they dream of commands that no sufficiently advanced mind can disobey.&nbsp; The gods themselves will proclaim the rightness of their philosophy!&nbsp; (E.g. John C. Wright, Marc Geddes.)</span></p>
<p><span style="font-style: normal;">There is also a less severe version of the failure, where the one does not </span><em>declare </em><span style="font-style: normal;">the One True Morality.&nbsp; Rather the one hopes for an AI created </span><em>perfectly free</em><span style="font-style: normal;">, unconstrained by flawed humans desiring slaves, so that the AI may arrive at virtue of its own accord&mdash;virtue undreamed-of perhaps by the speaker, who confesses themselves too flawed to teach an AI.&nbsp; (E.g. John K Clark, Richard Hollerith?, <a href="https://www.lesswrong.com/lw/iy/my_wild_and_reckless_youth/">Eliezer</a></span><a href="https://www.lesswrong.com/lw/iy/my_wild_and_reckless_youth/"><sub><span style="font-style: normal;">1996</span></sub></a><span style="font-style: normal;">.) This is a less tainted motive than the dream of absolute command. But though </span><em>this</em><span style="font-style: normal;"> dream arises from virtue rather than vice, it is still based on a flawed understanding of <a href="https://www.lesswrong.com/lw/rc/the_ultimate_source/">freedom</a>, and will not actually </span><em>work in real life.</em><span style="font-style: normal;">&nbsp; Of this, more to follow, of course.</span></p>
<p><span style="font-style: normal;">John C. Wright, who was previously writing a very nice transhumanist trilogy (first book:</span><em> The Golden Age</em><span style="font-style: normal;">) inserted a huge Author Filibuster in the middle of his climactic third book, describing in tens of pages his Universal Morality That Must Persuade Any AI.&nbsp; I don't know if anything happened after that, because I stopped reading.&nbsp; And then Wright converted to Christianity&mdash;yes, seriously.&nbsp; So you </span><em>really don't</em><span style="font-style: normal;"> want to fall into this trap!</span></p>
<hr />
<p><span style="font-size: 0.8em;">Footnote 1: Just kidding.</span></p>