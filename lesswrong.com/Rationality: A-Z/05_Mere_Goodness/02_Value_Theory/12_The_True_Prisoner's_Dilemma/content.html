<p>It occurred to me one day that the standard visualization of the <a href="http://en.wikipedia.org/wiki/Prisoner%27s_dilemma">Prisoner's Dilemma</a> is fake.</p>

<p>The core of the Prisoner's Dilemma is this symmetric payoff matrix:</p>



<table border="2"><tbody>
<tr><td></td>

<td>1: C</td>

<td>1:&nbsp; D</td></tr>

<tr>

<td>2: C</td>

<td>(3, 3)</td>

<td>(5, 0)</td></tr>

<tr><td>2: D</td>

<td>(0, 5)</td>

<td>(2, 2)</td></tr></tbody></table>

<p>Player 1, and Player 2, can each choose C or D.&nbsp; 1 and 2's utility for the final outcome is given by the first and second number in the pair.&nbsp; For reasons that will become apparent, &quot;C&quot; stands for &quot;cooperate&quot; and D stands for &quot;defect&quot;.</p>

<p>Observe that a player in this game (regarding themselves as the first player) has this preference ordering over outcomes:&nbsp; (D, C) &gt; (C, C) &gt; (D, D) &gt; (C, D).</p>

<p>D, it would seem, dominates C:&nbsp; If the other player chooses C, you prefer (D, C) to (C, C); and if the other player chooses D, you prefer (D, D) to (C, D).&nbsp; So you wisely choose D, and as the payoff table is symmetric, the other player likewise chooses D.</p>

<p>If only you'd both been less wise!&nbsp; You <em>both</em> prefer (C, C) to (D, D).&nbsp; That is, you both prefer mutual cooperation to mutual defection.</p>

<p>The Prisoner's Dilemma is one of the great foundational issues in decision theory, and enormous volumes of material have been written about it.&nbsp; Which makes it an audacious assertion of mine, that the usual way of <em>visualizing</em> the Prisoner's Dilemma has a severe flaw, at least if you happen to be human.</p><a id="more"></a><p>The classic visualization of the Prisoner's Dilemma is as follows: you
are a criminal, and you and your confederate in crime have both been
captured by the authorities.</p>

<p>Independently, without communicating, and
without being able to change your mind afterward, you have to decide
whether to give testimony against your confederate (D) or remain silent
(C).</p>

<p>Both of you, right now, are facing one-year prison sentences;
testifying (D) takes one year off your prison sentence, and adds two years
to your confederate's sentence.</p>

<p>Or maybe you and some stranger are, only once, and without knowing the other player's history, or finding out who the player was afterward, deciding whether to play C or D, for a payoff in dollars matching the standard chart.</p>

<p>And, oh yes - in the classic visualization you're supposed to <em>pretend that you're entirely
selfish</em>, that you don't care about your confederate criminal, or the player in
the other room.</p>

<p>It's this last specification that makes the classic visualization, in my view, fake.</p>

<p>You <a href="/lw/il/hindsight_bias/">can't avoid hindsight bias</a> by instructing a jury to pretend not to know the real outcome of a set of events.&nbsp; And without a complicated effort backed up by considerable knowledge, a neurologically intact human being cannot pretend to be genuinely, truly selfish.</p>

<p>We're born with a sense of fairness, honor, empathy, sympathy, and even altruism - the result of our ancestors <a href="/lw/l0/adaptationexecuters_not_fitnessmaximizers/">adapting</a> to play the <em>iterated</em> Prisoner's Dilemma.&nbsp; We don't really, truly, absolutely and entirely prefer (D, C) to (C, C), though we may entirely prefer (C, C) to (D, D) and (D, D) to (C, D).&nbsp; The thought of our confederate spending three years in prison, does not entirely fail to move us.</p>

<p>In that locked cell where we play a simple game under the supervision of economic psychologists, we are not entirely and absolutely unsympathetic for the stranger who might cooperate.&nbsp; We aren't entirely happy to think what we might defect and the stranger cooperate, getting five dollars while the stranger gets nothing.</p>

<p>We fixate instinctively on the (C, C) outcome and search for ways to argue that it should be the mutual decision:&nbsp; &quot;How can we ensure mutual cooperation?&quot; is the instinctive thought.&nbsp; Not &quot;How can I trick the other player into playing C while I play D for the maximum payoff?&quot;</p>

<p>For someone with an impulse toward altruism, or honor, or fairness, the Prisoner's Dilemma doesn't <em>really</em> have the critical payoff matrix - whatever the <em>financial</em> payoff to individuals.&nbsp; (C, C) &gt; (D, C), and the key question is whether the other player sees it the same way.</p>

<p>And no, you can't instruct people being initially introduced to game theory to pretend they're completely selfish - any more than you can instruct human beings being introduced to <a href="/lw/st/anthropomorphic_optimism/">anthropomorphism</a> to pretend they're expected paperclip maximizers.</p>

<p>To construct the True Prisoner's Dilemma, the situation has to be something like this:</p>

<p>Player 1:&nbsp; Human beings, Friendly AI, or other humane intelligence.</p>

<p>Player 2:&nbsp; UnFriendly AI, or an alien that <a href="/lw/sy/sorting_pebbles_into_correct_heaps/">only cares about sorting pebbles</a>.</p>

<p>Let's suppose that four billion human beings - not the whole human species, but a significant part of it - are currently progressing through a fatal disease that can only be cured by substance S.</p>

<p>However, substance S can only be produced by working with a paperclip maximizer from another dimension - substance S can also be used to produce paperclips.&nbsp; The paperclip maximizer only cares about the number of paperclips in its own universe, not in ours, so we can't offer to produce or threaten to destroy paperclips here.&nbsp; We have never interacted with the paperclip maximizer before, and will never interact with it again.</p>

<p>Both humanity and the paperclip maximizer will get a single chance to seize some additional part of substance S for themselves, just before the dimensional nexus collapses; but the seizure process destroys some of substance S.</p>

<p>The payoff matrix is as follows:</p>

<table border="2"><tbody>
<tr><td></td>

<td>1: C</td>

<td>1:&nbsp; D</td></tr>

<tr>

<td>2: C</td>

<td>(2 billion human lives saved, 2 paperclips gained)</td>

<td>(+3 billion lives, +0 paperclips)</td></tr>

<tr><td>2: D</td>

<td>(+0 lives, +3 paperclips)</td>

<td>(+1 billion lives, +1 paperclip)</td></tr></tbody></table>

<p>I've chosen this payoff matrix to produce a sense of <em>indignation</em> at the thought that the paperclip maximizer wants to trade off billions of human lives against a couple of paperclips.&nbsp; Clearly the paperclip maximizer <em>should</em> just let us have all of substance S; but a paperclip maximizer doesn't do what it <em>should,</em> it just maximizes paperclips.</p>

<p>In this case, we <em>really do</em> prefer the outcome (D, C) to the outcome (C, C), leaving aside the actions that produced it.&nbsp; We would vastly rather live in a universe where 3 billion humans were cured of their disease and no paperclips were produced, rather than sacrifice a billion human lives to produce 2 paperclips.&nbsp; It doesn't seem <em>right</em> to cooperate, in a case like this.&nbsp; It doesn't even seem <em>fair</em> - so great a sacrifice by us, for so little gain by the paperclip maximizer?&nbsp; And let us specify that the paperclip-agent experiences no pain or pleasure - it just outputs actions that steer its universe to contain more paperclips.&nbsp; The paperclip-agent will experience no pleasure at gaining paperclips, no hurt from losing paperclips, and no painful sense of betrayal if we betray it.</p>

<p>What do you do then?&nbsp; Do you
cooperate when you really, definitely, truly and absolutely do want the
highest reward you can get, and you don't care a tiny bit by comparison
about what happens to the other player?&nbsp; When it seems <em>right</em> to defect even if the other player cooperates?</p>

<p>That's what the
payoff matrix for the <em>true</em> Prisoner's Dilemma looks like - a situation where (D, C) seems <em>righter</em> than (C, C).</p>

<p>But all the rest
of the logic - everything about what happens if both agents think that
way, and both agents defect - is the same.&nbsp; For the paperclip maximizer cares as little about human deaths, or human pain, or a human sense of betrayal, as we care about paperclips.&nbsp; Yet we both prefer (C, C) to (D, D).</p>

<p>So if you've ever prided yourself on cooperating in the Prisoner's Dilemma... or questioned the verdict of classical game theory that the &quot;<a href="/lw/nc/newcombs_problem_and_regret_of_rationality/">rational</a>&quot; choice is to defect... then what do you say to the True Prisoner's Dilemma above?</p>