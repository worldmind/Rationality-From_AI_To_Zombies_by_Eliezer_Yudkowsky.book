<p>Every now and then, you run across someone who has discovered the
One Great Moral Principle, of which all other values are a mere
derivative consequence.</p>

<p>I run across more of these people than you do.&nbsp; Only in my case, it's people who know <em>the amazingly simple utility function that is all you need to program into an artificial superintelligence</em> and then everything will turn out fine.</p>

<p>(This post <em>should</em> come as an anticlimax, since you already know virtually all the concepts involved, <em>I bloody well hope.</em>&nbsp; See <a href="https://www.lesswrong.com/lw/lp/fake_fake_utility_functions/">yesterday's post</a>, and all my posts since <a href="https://www.lesswrong.com/lw/kq/fake_justification/">October 31st</a>, actually...)</p><a id="more"></a>
<p>Some people, when they encounter the how-to-program-a-superintelligence problem, try to solve the problem immediately.&nbsp; Norman R. F. Maier:&nbsp; &quot;<a href="https://www.lesswrong.com/lw/ka/hold_off_on_proposing_solutions/">Do not propose solutions</a>
until the problem has been discussed as thoroughly as possible without
suggesting any.&quot;&nbsp; Robyn Dawes:&nbsp; &quot;I have often used this edict
with groups I have led - particularly when they face a very tough
problem, which is when group members are most apt to propose solutions
immediately.&quot;&nbsp; Friendly AI is an <em>extremely</em> tough problem so people solve it <em>extremely</em> fast.

</p>

<p>There's several major classes of fast wrong solutions I've observed; and one of these is the Incredibly Simple Utility Function That Is All A Superintelligence Needs For Everything To Work Out Just Fine.</p>

<p>I may have contributed to this problem with a really poor choice of phrasing, years ago when I first started talking about &quot;Friendly AI&quot;.&nbsp; I referred to the <a href="https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/">optimization criterion</a> of an optimization process - the region into which an agent tries to steer the future - as the &quot;supergoal&quot;.&nbsp; I'd <a href="https://www.lesswrong.com/lw/ke/illusion_of_transparency_why_no_one_understands/">meant</a> &quot;super&quot; in the sense of &quot;parent&quot;, the source of a directed link in an acyclic graph.&nbsp; But it seems the effect of my phrasing was to send some people into <a href="https://www.lesswrong.com/lw/lm/affective_death_spirals/">happy death spirals</a> as they tried to imagine the Superest Goal Ever, the
Goal That Overrides All Over Goals, the Single Ultimate Rule From Which All
Ethics Can Be Derived.</p>

<p>But a utility function doesn't have to be simple.&nbsp; It can contain an arbitrary number of terms.&nbsp; We have every reason to believe that insofar as humans can said to be have values, there are lots of them - high <a href="https://www.lesswrong.com/lw/jp/occams_razor/">Kolmogorov complexity</a>.&nbsp; A human brain implements a thousand shards of desire, though this fact may not be appreciated by one who has not studied <a href="https://www.lesswrong.com/lw/l1/evolutionary_psychology/">evolutionary psychology</a>.&nbsp; (Try to explain this without a <a href="https://www.lesswrong.com/lw/kg/expecting_short_inferential_distances/">full, long introduction</a>, and the one hears &quot;humans are trying to maximize fitness&quot;, which is exactly the <a href="https://www.lesswrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/">opposite</a> of what evolutionary psychology says.)</p>

<p>So far as <a href="http://atheism.about.com/library/FAQs/phil/blfaq_phileth_cat.htm">descriptive</a> theories of morality are concerned, the complicatedness of human morality is a <em>known fact.</em>&nbsp; It is a <em>descriptive</em> fact about human beings, that the love of a parent for a child, and the love of a child for a parent, and the love of a man for a woman, and the love of a woman for a man, have not been cognitively derived from each other or from any other value.&nbsp; A mother doesn't have to do complicated moral philosophy to love her daughter, nor extrapolate the consequences to some other desideratum.&nbsp; There are many such shards of desire, all <em>different</em> values.</p>

<p>Leave out just <em>one</em> of these values from a superintelligence, and even if you successfully include <em>every other</em> value, you could end up with a <a href="http://www.nickbostrom.com/existential/risks.html">hyperexistential catastrophe</a>, a fate worse than death.&nbsp; If there's a superintelligence that wants everything for us that we want for ourselves, <em>except</em> the human values relating to controlling your own life and achieving your own goals, that's one of the oldest dystopias in the <a href="https://www.lesswrong.com/lw/k9/the_logical_fallacy_of_generalization_from/">book</a>.&nbsp; (Jack Williamson's &quot;With Folded Hands&quot;, in this case.)</p>

<p>So how does the one constructing the Amazingly Simple Utility Function deal with this objection?</p>

<p dragover="true">Objection?&nbsp; <em>Objection?</em>&nbsp; Why would they be searching for possible <em>objections</em> to their lovely theory?&nbsp; (Note that the process of searching for <a href="https://www.lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/">real, fatal objections</a> isn't the same as performing a <a href="https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/">dutiful search</a> that amazingly hits on only questions to which they have a snappy answer.)&nbsp; They don't know any of this stuff.&nbsp; They aren't thinking about burdens of proof.&nbsp; They don't know the problem is difficult.&nbsp; They heard the word &quot;supergoal&quot; and went off in a <a href="https://www.lesswrong.com/lw/lm/affective_death_spirals/">happy death spiral</a> around &quot;<a href="https://www.lesswrong.com/lw/ix/say_not_complexity/">complexity</a>&quot; or whatever.</p>



<p>Press them on some particular point, like the love a mother has for her children, and they reply &quot;But if the superintelligence wants 'complexity', it will see how complicated the parent-child relationship is, and therefore encourage mothers to love their children.&quot;&nbsp; Goodness, where do I start?</p>

<p>Begin with the <a href="https://www.lesswrong.com/lw/km/motivated_stopping_and_motivated_continuation/">motivated stopping</a>:&nbsp; A superintelligence actually searching for ways to maximize complexity wouldn't conveniently stop if it noticed that a parent-child relation was complex.&nbsp; It would ask if anything else was <em>more</em> complex.&nbsp; This is a <a href="https://www.lesswrong.com/lw/kq/fake_justification/">fake justification</a>; the one trying to argue the imaginary superintelligence into a policy selection, didn't really arrive at that policy proposal by carrying out a <a href="https://www.lesswrong.com/lw/kz/fake_optimization_criteria/">pure search</a> for ways to maximize complexity.</p>

<p>The whole argument is a <a href="https://www.lesswrong.com/lw/ky/fake_morality/">fake morality</a>.&nbsp; If what you <em>really</em>
valued was complexity, then you would be justifying the parental-love
drive by pointing to how it increases complexity.&nbsp; If you justify a
complexity drive by alleging that it increases parental love, it means
that what you really value is the parental love.&nbsp; It's like giving a
prosocial argument in favor of selfishness.</p>

<p>But if you consider the affective death spiral, then it doesn't increase the perceived niceness of &quot;complexity&quot; to say &quot;A mother's relationship to her daughter is only important because it increases complexity; consider that if the relationship became simpler, we would not value it.&quot;&nbsp; What does increase the perceived niceness of &quot;complexity&quot; is saying, &quot;If you set out to increase complexity, mothers will love their daughters - look at the positive consequence this has!&quot;<br />
</p>

<p>This point applies whenever you run across a moralist who tries to convince you that their One Great Idea is all that anyone needs for moral judgment, and proves this by saying, &quot;Look at all these positive consequences of this Great Thingy&quot;, rather than saying, &quot;Look at how all these things we think of as 'positive' are only positive when their consequence is to increase the Great Thingy.&quot;&nbsp; The latter being what you'd actually need to carry such an argument.</p>

<p>But if you're trying to persuade others (or yourself) of your theory that the One Great Idea is &quot;bananas&quot;, you'll sell a lot more bananas by arguing how bananas lead to better sex, rather than claiming that you should only want sex when it leads to bananas.</p>

<p>Unless you're so far gone into the Happy Death Spiral that you really <em>do</em> start saying &quot;Sex is only good when it leads to bananas.&quot;&nbsp; Then you're in trouble.&nbsp; But at least you won't convince anyone else.</p>

<p>In the end, the only process that reliably <a href="https://www.lesswrong.com/lw/la/truly_part_of_you/">regenerates</a> all the local decisions you would make given your morality, is your morality.&nbsp; Anything else - any attempt to substitute instrumental means for terminal ends - ends up <a href="https://www.lesswrong.com/lw/le/lost_purposes/">losing purpose</a> and requiring <a href="https://www.lesswrong.com/lw/l9/artificial_addition/">an infinite number of patches</a> because the system doesn't <a href="https://www.lesswrong.com/lw/la/truly_part_of_you/">contain the source</a> of the instructions you're giving it.&nbsp; You shouldn't expect to be able to compress a human morality down to a simple utility function, any more than you should expect to compress a large computer file down to 10 bits.</p>

<p><strong>Addendum:</strong>&nbsp; Please note that we're not yet ready to discuss Friendly AI, as such, on <em>Overcoming Bias.</em>&nbsp; That will require <em>a lot more</em> prerequisite material.&nbsp; This post is <em>only </em>about why simple utility functions fail to compress our values.</p>