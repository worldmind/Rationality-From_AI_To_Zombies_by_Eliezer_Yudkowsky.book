<p>When I met the futurist Greg Stock some years ago, he argued that the joy of scientific discovery would soon be replaced by pills that could simulate the joy of scientific discovery.&nbsp; I approached him after his talk and said, &quot;I agree that such pills are probably possible, but I wouldn't voluntarily <em>take them.</em>&quot;</p>

<p>And Stock said, &quot;But they'll be so much better that <a href="https://www.lesswrong.com/lw/h3/superstimuli_and_the_collapse_of_western/">the real thing won't be able to compete</a>.&nbsp; It will just be way more fun for you to take the pills than to do all the actual scientific work.&quot;</p>

<p>And I said, &quot;I <em>agree</em> that's possible, so I'll make sure never to take them.&quot;</p>

<p>Stock seemed genuinely surprised by my attitude, which genuinely surprised <em>me.</em></p><a id="more"></a><p>One often sees ethicists arguing as if all human desires are reducible, in principle, to the desire for ourselves and others to be happy.&nbsp; (In particular, Sam Harris does this in <em>The End of Faith,</em> which I just finished perusing - though Harris's reduction is more of a drive-by shooting than a major topic of discussion.)</p>

<p>This isn't the same as arguing whether <a href="https://www.lesswrong.com/lw/kn/torture_vs_dust_specks/">all happinesses can be measured on a common utility scale</a> - different happinesses might occupy different scales, or be otherwise non-convertible.&nbsp; And it's not the same as <a href="https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/">arguing that it's theoretically impossible to value anything other than your own psychological states</a>, because it's still permissible to care whether <em>other</em> people are happy.</p>

<p>The question, rather, is whether we <em>should</em> care about the things that <em>make</em> us happy, apart from any happiness they bring.</p>

<p>We can easily list many cases of moralists going astray by caring about things besides happiness.&nbsp; The various states and countries that still outlaw oral sex make a good example; these legislators would have been better off if they'd said, &quot;Hey, whatever turns you on.&quot;&nbsp; But this doesn't show that <em>all</em> values are reducible to happiness; it just argues that in <em>this particular</em> <em>case </em>it was an ethical mistake to focus on anything else.</p>

<p>It is an undeniable fact that we tend to do things that make us happy, but this doesn't mean we should regard the happiness as the <em>only</em> reason for so acting.&nbsp; First, this would make it difficult to explain how we could care about anyone else's happiness - how we could treat people as ends in themselves, rather than instrumental means of obtaining a warm glow of satisfaction.</p>

<p dragover="true">Second, just because something is a consequence of my action doesn't mean it was the sole justification.&nbsp; If I'm writing a blog post, and I get a headache, I may take an ibuprofen.&nbsp; <em>One</em> of the consequences of my action is that I experience less pain, but this doesn't mean it was the <em>only</em> consequence, or even the most important reason for my decision.&nbsp; I do value the state of not having a headache.&nbsp; But I can value something for its own sake <em>and also</em> value it as a means to an end.</p>

<p dragover="true">For all value to be reducible to happiness, it's not enough to show that happiness is involved in most of our decisions - it's not even enough to show that happiness is the <em>most</em> important consequent in <em>all</em> of our decisions - it must be the <em>only</em> consequent.&nbsp; That's a tough standard to meet.&nbsp; (I originally found this point in a Sober and Wilson paper, not sure which one.)</p>

<p>If I claim to value art for its own sake, then would I value art that no one ever saw?&nbsp; A screensaver running in a closed room, producing beautiful pictures that no one ever saw?&nbsp; I'd have to say no.&nbsp; I can't think of any completely lifeless object that I would value as an end, not just a means.&nbsp; That would be like valuing ice cream as an end in itself, apart from anyone eating it.&nbsp; Everything I value, that I can think of, involves people and their experiences <em>somewhere</em> along the line.</p>

<p>The best way I can put it, is that my moral intuition appears to require <em>both</em> the objective and subjective component to grant full value.</p>

<p>The value of scientific discovery requires <em>both</em> a genuine scientific discovery, and a person to take joy in that discovery.&nbsp; It may seem difficult to disentangle these values, but the pills make it clearer.</p>

<p>I would be disturbed if people retreated into holodecks and fell in love with mindless wallpaper.&nbsp; I would be disturbed <em>even if they weren't aware it was a holodeck</em>, which is an important ethical issue if some agents can potentially transport people into holodecks and substitute zombies for their loved ones without their awareness.&nbsp; Again, the pills make it clearer:&nbsp; I'm not just concerned with my own awareness of the uncomfortable fact.&nbsp; I wouldn't put myself into a holodeck even if I could take a pill to forget the fact afterward.&nbsp; That's simply not where I'm trying to steer the future.</p>

<p>I value freedom:&nbsp; When I'm deciding where to steer the future, I take into account not only the subjective states that people end up in, but also whether they got there as a result of their own efforts.&nbsp; The presence or absence of an external puppet master can affect my valuation of an otherwise fixed outcome.&nbsp; Even if people wouldn't know they were being manipulated, it would matter to my judgment of how well humanity had done with its future.&nbsp; This is an important ethical issue, if you're dealing with agents powerful enough to helpfully tweak people's futures without their knowledge.</p>

<p>So my values are not strictly reducible to happiness:&nbsp; There are properties I value about the future that aren't reducible to activation levels in anyone's pleasure center; properties that are not <em>strictly</em> reducible to subjective states even in principle.</p>

<p>Which means that my decision system has a <em>lot</em> of <a href="https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/">terminal values</a>, none of them strictly reducible to <a href="https://www.lesswrong.com/lw/l3/thou_art_godshatter/">anything else</a>.&nbsp; Art, science, love, lust, freedom, friendship...</p>

<p>And I'm okay with that.&nbsp; I value a life complicated enough to be challenging and aesthetic - not just the <em>feeling</em> that life is complicated, but the <em>actual</em> complications - so turning into a pleasure center in a vat doesn't appeal to me.&nbsp; It would be a waste of humanity's potential, which I value actually fulfilling, not just having the feeling that it was fulfilled.</p>