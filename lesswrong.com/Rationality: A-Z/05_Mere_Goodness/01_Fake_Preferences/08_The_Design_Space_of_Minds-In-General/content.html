<p style="font-style: normal;">People ask me, &quot;What will
Artificial Intelligences be like?&nbsp; What will they do?&nbsp; Tell us your
amazing story about the future.&quot;</p>
<p style="font-style: normal;">And lo, I say unto them, &quot;You have
asked me a trick question.&quot;</p>
<p>ATP
synthase is a molecular machine - one of three known occasions when
evolution has invented the freely rotating wheel - which is
essentially the same in animal mitochondria, plant chloroplasts, and
bacteria.&nbsp; ATP synthase has not changed significantly since the rise
of eukaryotic life two billion years ago.&nbsp; It's is something we <em>all</em><span style="font-style: normal;">
have in common -&nbsp; thanks to the way that evolution <a href="https://www.lesswrong.com/lw/rl/the_psychological_unity_of_humankind/">strongly conserves
certain genes</a>; once many other genes depend on a gene, a mutation
will tend to break all the dependencies.<br /></span></p>
<p style="font-style: normal;">Any two AI
designs might be less similar to each other than you are to a
petunia.</p><a id="more"></a><p><span style="font-style: normal;">Asking
what &quot;AIs&quot; will do is a trick question because it implies
that all AIs form a <a href="https://www.lesswrong.com/lw/nj/similarity_clusters/">natural class</a>. 
Humans do form a natural class because we all share the same brain
architecture.&nbsp; But when you say &quot;Artificial Intelligence&quot;,
you are referring to a vastly larger </span><em>space
of possibilities</em><span style="font-style: normal;">
than when you say &quot;human&quot;.&nbsp; When people talk about
&quot;AIs&quot; we are really talking about </span><em>minds-in-general,</em><span style="font-style: normal;">
or optimization processes in general.&nbsp; Having a word for &quot;AI&quot;
is like having a word for everything that isn't a duck.</span></p>
<p style="font-style: normal;">Imagine
a map of mind design space... this is one of my standard diagrams...</p>
<p><a href="../../../../book.english/img/fb27527df0c30f1101d0da43d0abe7b0.png" onclick="window.open(this.href, '_blank', 'width=800,height=800,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0'); return false"><img width="536" height="536" border="0" alt="Mindspace_2" title="Mindspace_2" src="../../../../book.english/img/fb27527df0c30f1101d0da43d0abe7b0.png" /></a></p>

<p>All humans, of course, fit into a tiny little dot - as a sexually reproducing species, <a href="https://www.lesswrong.com/lw/rl/the_psychological_unity_of_humankind/">we can't be too different from one another</a>.

</p>

<p><span style="font-style: normal;">This
tiny dot belongs to a wider ellipse, the space of transhuman mind
designs - things that might be smarter than us, or much smarter than
us, but which in some sense would still be people as we understand
people.</span></p>

<p><span style="font-style: normal;">This transhuman ellipse is within a still wider volume, the space of posthuman
minds, which is everything that a transhuman might grow up into.</span></p>

<p><span style="font-style: normal;">And
then the rest of the sphere is the space of minds-in-general,
including possible Artificial Intelligences so odd that they aren't
even </span><em>posthuman.</em></p>
<p style="font-style: normal;">But
wait - natural selection designs complex artifacts and selects among
complex strategies.&nbsp; So where is natural selection on this map?</p>

<p style="font-style: normal;">So
this entire map really floats in a still vaster space, the space of
optimization processes.&nbsp; At the bottom of this vaster space, below
even humans, is natural selection as it first began in some tidal
pool: mutate, replicate, and sometimes die, no sex.</p>
<p style="font-style: normal;">Are
there any powerful optimization processes, with strength comparable
to a human civilization or even a self-improving AI, which we would
not recognize as minds?&nbsp; Arguably <a href="http://www.hutter1.net/ai/">Marcus Hutter's
AIXI</a> should go in this category: for a mind of infinite power, it's
awfully stupid - poor thing can't even recognize itself in a mirror.&nbsp; 
But that is a topic for another time.</p>
<p><span style="font-style: normal;">My
primary moral is to </span><em>resist
the temptation to generalize over all of mind design space</em></p>

<p><span style="font-style: normal;">If we focus on the bounded subspace of mind design space which contains all those minds whose
makeup can be specified in a trillion bits or less, then every
universal generalization that you make has two to the trillionth
power chances to be falsified.</span></p>

<p><span style="font-style: normal;">Conversely, every </span><em>existential</em><span style="font-style: normal;">
generalization - &quot;there exists at least one mind such that X&quot;
- has two to the trillionth power chances to be true.</span></p>

<p><span style="font-style: normal;">So you want to
resist the temptation to say either that </span><em>all</em><span style="font-style: normal;">
minds do something, or that </span><em>no</em><span style="font-style: normal;">
minds do something.</span></p>
<p><span style="font-style: normal;">The
main reason you could find yourself thinking that you know what a
fully generic mind will (won't) do, is if you put yourself in that mind's
shoes - imagine what you would do in that mind's place - and get back
a generally wrong, anthropomorphic answer.&nbsp; (Albeit that it is true in at least one case, since you are yourself an example.)&nbsp; Or if you imagine a mind
doing something, and then imagining the reasons </span><em>you</em><span style="font-style: normal;">
wouldn't do it - so that you imagine that a mind of that type can't
exist, that the <a href="https://www.lesswrong.com/lw/rf/ghosts_in_the_machine/">ghost in the machine</a> will look over the corresponding
source code and hand it back.</span></p>

<p><span style="font-style: normal;">Somewhere in mind design space is at least one mind with almost any kind of logically consistent property you care to imagine.</span></p>

<p><span style="font-style: normal;">And this is important because it emphasizes the importance of discussing <em>what happens, lawfully, and why,</em> as a causal result of a mind's particular constituent makeup; somewhere in mind design space is a mind that does it differently.<br /></span></p>

<p>Of course you could always say that anything which doesn't do it your way, is &quot;<a href="https://www.lesswrong.com/lw/nz/arguing_by_definition/">by definition</a>&quot; not a mind; after all, it's obviously stupid<span style="font-style: normal;">.&nbsp; I've seen people try that one too.</span></p>
